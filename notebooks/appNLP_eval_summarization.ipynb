{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization ROUGE scoring script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script contains the source code of how to **evaluate** a t5 model fine-tuned on **End-to-End automatic summarization task** with ROUGE scores. The scoring is multi-lingual.\n",
    "\n",
    "The ROUGE scoring process is separated from the finetuning/applying script. \n",
    "Suppose the apply function generates \"extracted_summary\" column, run the following bolded SPL to calculate ROUGEs\n",
    "\n",
    "Parameter *metrics* indicates the ROUGE score type, selected from { *rouge1_fmeasure*, *rouge1_precision*, *rouge1_recall*, *rouge2_fmeasure*, *rouge2_precision*, *rouge2_recall*, *rougeL_fmeasure*, *rougeL_precision*, *rougeL_recall*, *rougeLsum_fmeasure*, *rougeLsum_precision*, *rougeLsum_recall* }\n",
    "\n",
    "## SPL example:\n",
    "| inputlookup customer_support_en\n",
    "| head 3\n",
    "| apply t5_summarization_en_finetuned_final\n",
    "\n",
    "**| fields summary extracted_summary\n",
    "| fit MLTKContainer algo=appNLP_eval_summarization metrics=rouge1_fmeasure extracted_summary from summary into app:eval_summarization_scoring as rouge_score**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 0 - import libraries\n",
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "name": "mltkc_import"
   },
   "outputs": [],
   "source": [
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import copy\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics.text.rouge import ROUGEScore\n",
    "# tensorboard related\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tensorboard\n",
    "import datetime\n",
    "import logging\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "import psutil\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1 - get a data sample from Splunk\n",
    "In Splunk run a search to pipe a prepared dataset into this environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "name": "mltkc_stage"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG stage call\n",
      "DEBUGsummarization_eval_scoring\n"
     ]
    }
   ],
   "source": [
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",
    "def stage(name):\n",
    "    with open(\"/srv/notebooks/data/\"+name+\".csv\", 'r') as f:\n",
    "        df = pd.read_csv(f)\n",
    "    with open(\"/srv/notebooks/data/\"+name+\".json\", 'r') as f:\n",
    "        param = json.load(f) \n",
    "#         param = {}\n",
    "    return df, param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 - create and initialize a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "name": "mltkc_init"
   },
   "outputs": [],
   "source": [
    "def init(df,param):\n",
    "    model = {}\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3 - fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "name": "mltkc_fit"
   },
   "outputs": [],
   "source": [
    "# No model loaded. Stage holder\n",
    "def fit(model,df,param):  \n",
    "    returns = {}\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4 - apply the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "name": "mltkc_apply"
   },
   "outputs": [],
   "source": [
    "def apply(model,df,param):\n",
    "    tag = \"-- process=summarization_evaluation \"\n",
    "    X = df[\"summary\"].values.tolist()\n",
    "    Y = df[\"extracted_summary\"].values.tolist()\n",
    "    temp_rouge=list()\n",
    "    rouge = ROUGEScore()\n",
    "    print(tag + \"scoring begins\")\n",
    "    for i in range(len(X)):\n",
    "        print(tag + \"scoring {}-th utterance over {}. {}% finished.\".format(i+1, len(X), round(i/len(X)*100)))\n",
    "        r = rouge(X[i], Y[i])\n",
    "        temp_rouge.append(round(r[param['options']['params']['metrics']].item(),2))\n",
    "    cols={param['options']['params']['metrics']: temp_rouge}\n",
    "    returns=pd.DataFrame(data=cols)\n",
    "    print(tag + \"scoring successfully finished\")\n",
    "        \n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: enter apply\n",
      "{'options': {'params': {'algo': 'appNLP_eval_summarization', 'mode': 'stage', 'metrics': 'rouge1_fmeasure'}, 'args': ['extracted_summary', 'summary'], 'target_variable': ['extracted_summary'], 'feature_variables': ['summary'], 'model_name': 'summarization_eval_scoring', 'output_name': 'rouge_score', 'algo_name': 'MLTKContainer', 'mlspl_limits': {'disabled': False, 'handle_new_cat': 'default', 'max_distinct_cat_values': '100', 'max_distinct_cat_values_for_classifiers': '100', 'max_distinct_cat_values_for_scoring': '100', 'max_fit_time': '600', 'max_inputs': '100000', 'max_memory_usage_mb': '4000', 'max_model_size_mb': '30', 'max_score_time': '600', 'use_sampling': '1'}, 'kfold_cv': None}, 'feature_variables': ['summary'], 'target_variables': ['extracted_summary']}\n",
      "-- process=summarization_evaluation scoring begins\n",
      "-- process=summarization_evaluation scoring 1-th utterance over 1. 0% finished.\n",
      "-- process=summarization_evaluation scoring function successfully finished\n"
     ]
    }
   ],
   "source": [
    "returns = apply(None,df,param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 5 - save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "name": "mltkc_save"
   },
   "outputs": [],
   "source": [
    "# save model to name in expected convention \"<algo_name>_<model_name>.h5\"\n",
    "def save(model, name):\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 6 - load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "name": "mltkc_load"
   },
   "outputs": [],
   "source": [
    "# load model from name in expected convention \"<algo_name>_<model_name>.h5\"\n",
    "def load(path):\n",
    "    model = {}\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 7 - provide a summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "name": "mltkc_summary"
   },
   "outputs": [],
   "source": [
    "# return model summary\n",
    "def summary(model=None):\n",
    "    returns = {}\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Stages\n",
    "All subsequent cells are not tagged and can be used for further freeform code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
