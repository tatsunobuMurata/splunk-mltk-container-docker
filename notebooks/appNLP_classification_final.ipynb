{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification fine-tuning script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script contains the source code for ine-tuning the BERT model on **End-to-End text summarization task** in both English and Japanese."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 0 - import libraries\n",
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "name": "mltkc_import"
   },
   "outputs": [],
   "source": [
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",
    "# import sys\n",
    "# sys.path.insert(1, '/opt/conda/lib/python3.8/site-packages')\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "# import neologdn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "# tensorboard related\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tensorboard\n",
    "import datetime\n",
    "import logging\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "import psutil\n",
    "import shutil\n",
    "\n",
    "# Fine-tune parameters initialization\n",
    "MODEL_NAME = \"/srv/app/model/data\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "max_length_src = 500\n",
    "max_length_target = 200\n",
    "\n",
    "batch_size_train = 4\n",
    "batch_size_valid = 4\n",
    "\n",
    "epochs = 100\n",
    "patience = 20\n",
    "\n",
    "MODEL_DIRECTORY = \"/\"\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "        Bert Model for classification Tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, MODEL_NAME, D_out, freeze_bert=False):\n",
    "        super(BertClassifier,self).__init__()\n",
    "        D_in, H, D_out = 768, 60, D_out\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.classifier = nn.Sequential(\n",
    "                            nn.Linear(D_in, H),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(H, D_out))\n",
    "        # Freeze the Bert Model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "    def forward(self,input_ids,attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                           attention_mask = attention_mask)\n",
    "        last_hidden_state_cls = outputs[0][:,0,:]\n",
    "        logit = self.classifier(last_hidden_state_cls)\n",
    "        \n",
    "        return logit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1 - get a data sample from Splunk\n",
    "In Splunk run a search to pipe a prepared dataset into this environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| inputlookup classification_en\n",
    "| fields - TITLE ID\n",
    "| rename ABSTRACT as text\n",
    "| head 100\n",
    "| fit MLTKContainer algo=appNLP_classification_test mode=stage \"Computer Science\" max_epochs=1 lang=en base_model=bert_classification_en text \"Mathematics\" \"Physics\" \"Quantitative Biology\" \"Quantitative Finance\" \"Statistics\"  into app:bert_classification_en_finetuned_test as score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "name": "mltkc_stage"
   },
   "outputs": [],
   "source": [
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",
    "def stage(name):\n",
    "    print(\"DEBUG stage call\")\n",
    "    print(\"DEBUG\" + name)\n",
    "    with open(\"/srv/notebooks/data/\"+name+\".csv\", 'r') as f:\n",
    "        df = pd.read_csv(f)\n",
    "    with open(\"/srv/notebooks/data/\"+name+\".json\", 'r') as f:\n",
    "        param = json.load(f) \n",
    "#         param = {}\n",
    "    return df, param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG stage call\n",
      "DEBUGbert_classification_en_finetuned_test\n"
     ]
    }
   ],
   "source": [
    "df, param = stage(\"bert_classification_en_finetuned_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 - create and initialize a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "name": "mltkc_init"
   },
   "outputs": [],
   "source": [
    "def init(df,param):\n",
    "    tag = \"-- process=fine_tuning_progress model={} max_epoch={} -- \".format(param['options']['params']['base_model'], param['options']['params']['max_epochs'])\n",
    "#     df, param = df, param\n",
    "#     if df is not None:\n",
    "#         print(\"received df\")\n",
    "#     if param is not None:\n",
    "#         print(\"received param\")\n",
    "#         print(param)\n",
    "#     print(\"DEBUG init call\")\n",
    "\n",
    "    print(tag + \"Training data loaded with shape: \" + str(df.shape))\n",
    "    print(tag + \"Input parameters: \", param['options']['params'])\n",
    "    print(tag + \"Epoch number: \" + param['options']['params']['max_epochs'])\n",
    "    print(tag + \"Base model: \" + param['options']['params']['base_model'])\n",
    "    \n",
    "#     logging.info(param['options']['params']['base_model'])\n",
    "    # Load English parser and text blob (for sentiment analysis)\n",
    "#     model = {}\n",
    "    print(tag + \"Model Initialization: started\")\n",
    "    l = len(list(df.columns)) - 1\n",
    "    MODEL_NAME = \"/srv/app/model/data/classification\"\n",
    "    MODEL_NAME = os.path.join(MODEL_NAME, param['options']['params']['lang'], param['options']['params']['base_model'])\n",
    "    print(tag + \"Model file in \" + MODEL_NAME)\n",
    "#     if param['options']['params']['lang'] == \"jp\":\n",
    "    model = BertClassifier(MODEL_NAME, l)\n",
    "    model = model.to(device)\n",
    "    print(tag + \"Model Initialization: successfully finished\")\n",
    "    # GPU memory calculation\n",
    "    t = torch.cuda.get_device_properties(0).total_memory\n",
    "    r = torch.cuda.memory_reserved(0)\n",
    "    a = torch.cuda.memory_allocated(0)\n",
    "    f = r-a  # free inside reserved\n",
    "    load1, load5, load15 = psutil.getloadavg()\n",
    "    cpu_usage = (load15/os.cpu_count()) * 100\n",
    "    stat = shutil.disk_usage(\"/\")\n",
    "    \n",
    "    print(tag + \"#GPU memory --Total memory: {}, --Memory reserved: {}, --Memory allocated: {}. #CPU: {}% occupied. #disk {}\".format(t,r,a,cpu_usage,stat))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- process=fine_tuning_progress model=bert_classification_en max_epoch=2 -- Training data loaded with shape: (100, 7)\n",
      "-- process=fine_tuning_progress model=bert_classification_en max_epoch=2 -- Input parameters:  {'algo': 'appNLP_classification_test', 'mode': 'stage', 'max_epochs': '2', 'lang': 'en', 'base_model': 'bert_classification_en'}\n",
      "-- process=fine_tuning_progress model=bert_classification_en max_epoch=2 -- Epoch number: 2\n",
      "-- process=fine_tuning_progress model=bert_classification_en max_epoch=2 -- Base model: bert_classification_en\n",
      "-- process=fine_tuning_progress model=bert_classification_en max_epoch=2 -- Model Initialization: started\n",
      "-- process=fine_tuning_progress model=bert_classification_en max_epoch=2 -- Model file in /srv/app/model/data/classification/en/bert_classification_en\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- process=fine_tuning_progress model=bert_classification_en max_epoch=2 -- Model Initialization: successfully finished\n",
      "-- process=fine_tuning_progress model=bert_classification_en max_epoch=2 -- #GPU memory --Total memory: 15634661376, --Memory reserved: 494927872, --Memory allocated: 439256576. #CPU: 0.75% occupied. #disk usage(total=156052275200, used=154390712320, free=1644785664)\n"
     ]
    }
   ],
   "source": [
    "model = init(df,param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3 - fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "name": "mltkc_fit"
   },
   "outputs": [],
   "source": [
    "def fit(model,df,param):  \n",
    "    tag = \"-- process=fine_tuning_progress model={} max_epoch={} -- \".format(param['options']['params']['base_model'], param['options']['params']['max_epochs'])\n",
    "    l = len(list(df.columns)) - 1\n",
    "    MODEL_DIRECTORY = os.path.join(\"/srv/app/model/data/classification\", param['options']['params']['lang'],param['options']['model_name'])\n",
    "    if \"batch_size\" in param['options']['params']:\n",
    "        print(tag + \"setting batch size to \", param['options']['params']['batch_size'])\n",
    "        batch_size_train = int(param['options']['params']['batch_size'])\n",
    "        batch_size_valid = int(param['options']['params']['batch_size'])\n",
    "    else:\n",
    "        batch_size_train = 4\n",
    "        batch_size_valid = 4\n",
    "    # Data preparation\n",
    "    def text_preprocessing(text):\n",
    "        if param['options']['params']['lang'] == \"en\":\n",
    "            text = text.lower()\n",
    "            text = re.sub(r\"what's\", \"what is \", text)\n",
    "            text = re.sub(r\"won't\", \"will not \", text)\n",
    "            text = re.sub(r\"\\'s\", \" \", text)\n",
    "            text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "            text = re.sub(r\"can't\", \"can not \", text)\n",
    "            text = re.sub(r\"n't\", \" not \", text)\n",
    "            text = re.sub(r\"i'm\", \"i am \", text)\n",
    "            text = re.sub(r\"\\'re\", \" are \", text)\n",
    "            text = re.sub(r\"\\'d\", \" would \", text)\n",
    "            text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "            text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "            text = re.sub(r\"\\'\\n\", \" \", text)\n",
    "            text = re.sub(r\"-\", \" \", text)\n",
    "            text = re.sub(r\"\\'\\xa0\", \" \", text)\n",
    "            text = re.sub('\\s+', ' ', text)\n",
    "            text = ''.join(c for c in text if not c.isnumeric())\n",
    "            text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
    "            text = re.sub(r'&amp;', '&', text)\n",
    "            text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        else:\n",
    "            text = re.sub(r'[\\r\\t\\n\\u3000]', '', text)\n",
    "            text = text.lower()\n",
    "            text = text.strip()\n",
    "        return text\n",
    "    \n",
    "    MODEL_NAME = \"/srv/app/model/data/classification\"\n",
    "    MODEL_NAME = os.path.join(MODEL_NAME, param['options']['params']['lang'], param['options']['params']['base_model'])\n",
    "    tokenizer = BertTokenizer.from_pretrained(MODEL_NAME,do_lower_case=True)\n",
    "    print(tag + \"tokenizer intialized\")\n",
    "\n",
    "    def preprocessing_for_bert(data):\n",
    "        input_ids = []\n",
    "        attention_masks = []   \n",
    "        for sent in data:\n",
    "            encoded_sent = tokenizer.encode_plus(\n",
    "            text = text_preprocessing(sent),   #preprocess sentence\n",
    "            add_special_tokens = True,         #Add `[CLS]` and `[SEP]`\n",
    "            max_length= max_length_src  ,             #Max length to truncate/pad\n",
    "            pad_to_max_length = True,          #pad sentence to max length \n",
    "            return_attention_mask= True        #Return attention mask \n",
    "            )\n",
    "            # Add the outputs to the lists\n",
    "            input_ids.append(encoded_sent.get('input_ids'))\n",
    "            attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "        \n",
    "        #convert lists to tensors\n",
    "        input_ids = torch.tensor(input_ids)\n",
    "        attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "        return input_ids,attention_masks\n",
    "    \n",
    "    labels = list(df.columns)[1:]\n",
    "    X = df.text.values\n",
    "    y = df[labels].values\n",
    "    X_train, X_val, y_train, y_val =train_test_split(X, y, test_size=0.1, random_state=42, shuffle=True)\n",
    "    print(tag + \"Data vectorization: started\")\n",
    "    train_inputs, train_masks = preprocessing_for_bert(X_train)\n",
    "    val_inputs, val_masks = preprocessing_for_bert(X_val)\n",
    "    train_labels = torch.tensor(y_train)\n",
    "    val_labels = torch.tensor(y_val)\n",
    "    \n",
    "    # Create the DataLoader for our training set\n",
    "    train_data = TensorDataset(train_inputs,train_masks, train_labels)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size_train)\n",
    "\n",
    "    # Create the DataLoader for our validation set\n",
    "    val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "    val_sampler = SequentialSampler(val_data)\n",
    "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size_valid)\n",
    "    print(tag + \"Data vectorization: finished.\")\n",
    "    print(tag + \"#Training data: \" + str(len(train_data)) + \", #Test data: \" + str(len(val_data)))\n",
    "\n",
    "    \n",
    "    def initialize_model(epochs=4):\n",
    "        \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
    "        \"\"\"\n",
    "\n",
    "        # Instantiate Bert Classifier\n",
    "        bert_classifier = model\n",
    "\n",
    "        # Create the optimizer\n",
    "        optimizer = AdamW(bert_classifier.parameters(),\n",
    "                         lr=5e-5, #Default learning rate\n",
    "                         eps=1e-8 #Default epsilon value\n",
    "                         )\n",
    "        # Total number of training steps\n",
    "        total_steps = len(train_dataloader) * epochs\n",
    "        # Set up the learning rate scheduler\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                  num_warmup_steps=0, # Default value\n",
    "                                                  num_training_steps=total_steps)\n",
    "        return bert_classifier, optimizer, scheduler\n",
    "    \n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def set_seed(seed_value=42):\n",
    "        \"\"\"Set seed for reproducibility.\n",
    "        \"\"\"\n",
    "        random.seed(seed_value)\n",
    "        np.random.seed(seed_value)\n",
    "        torch.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        \n",
    "    # Training function\n",
    "    def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
    "        \"\"\"Train the BertClassifier model.\n",
    "        \"\"\"\n",
    "        # Start training loop\n",
    "#         print(\"Start training...\\n\")\n",
    "        for epoch_i in range(epochs):\n",
    "            # =======================================\n",
    "            #               Training\n",
    "            # =======================================\n",
    "            # Print the header of the result table\n",
    "#             print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "#             print(\"-\"*70)\n",
    "\n",
    "            # Measure the elapsed time of each epoch\n",
    "            t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "            # Reset tracking variables at the beginning of each epoch\n",
    "            total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "            \n",
    "            total = len(train_dataloader)\n",
    "\n",
    "            # Put the model into the training mode\n",
    "            model.train()\n",
    "#             train_data = tqdm(train_dataloader, file=sys.stdout)\n",
    "            for step, batch in enumerate(train_dataloader):\n",
    "#                 train_data.set_description(tag)\n",
    "                batch_counts +=1\n",
    "                # Load batch to GPU\n",
    "                b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "                # Zero out any previously calculated gradients\n",
    "                model.zero_grad()\n",
    "\n",
    "                # Perform a forward pass. This will return logits.\n",
    "                logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "                # Compute loss and accumulate the loss values\n",
    "                loss = loss_fn(logits, b_labels.float())\n",
    "                batch_loss += loss.item()\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "#                 train_data.set_postfix(loss=total_loss / (step+1))\n",
    "\n",
    "                # Perform a backward pass to calculate gradients\n",
    "                loss.backward()\n",
    "\n",
    "                # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "                # Update parameters and the learning rate\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                \n",
    "                print(tag + \"Processed {}% of the {}-th epoch. Finished {} out of {} batches. Loss: {} \".format(round(batch_counts/total*100), epoch_i+1, batch_counts, total, round(batch_loss / batch_counts,2)), flush=True)\n",
    "                \n",
    "                if (step % 50000 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                    # Calculate time elapsed for 20 batches\n",
    "                    time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                    # Print training results\n",
    "#                     print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "                    \n",
    "                    # Reset batch tracking variables\n",
    "                    batch_loss, batch_counts = 0, 0\n",
    "                    t0_batch = time.time()\n",
    "\n",
    "            # Calculate the average loss over the entire training data\n",
    "            avg_train_loss = total_loss / len(train_dataloader)\n",
    "            \n",
    "#             print(\"-\"*70)\n",
    "            \n",
    "            tokenizer.save_pretrained(MODEL_DIRECTORY)\n",
    "            print(tag + \"tokenizer saved in \" + MODEL_DIRECTORY, flush=True)\n",
    "            torch.save(model.state_dict(),os.path.join(MODEL_DIRECTORY, \"pytorch_model.pt\"))\n",
    "            print(tag + \"model saved in \" + MODEL_DIRECTORY, flush=True)\n",
    "            # =======================================\n",
    "            #               Evaluation\n",
    "            # =======================================\n",
    "            if evaluation == True:\n",
    "                # After the completion of each training epoch, measure the model's performance\n",
    "                # on our validation set.\n",
    "                val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "\n",
    "                # Print performance over the entire training data\n",
    "                time_elapsed = time.time() - t0_epoch\n",
    "                \n",
    "                print(tag + '[{}/{}] train loss: {:.4f}, valid loss: {:.4f}, valid accuracy: {:.4f} [{}{:.0f}s]'.format(\n",
    "                        epoch_i, epochs, avg_train_loss, val_loss, val_accuracy,\n",
    "                        str(int(math.floor(time_elapsed / 60))) + 'm' if math.floor(time_elapsed / 60) > 0 else '',\n",
    "                        time_elapsed % 60\n",
    "                    ), flush=True)\n",
    "\n",
    "#                 print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "#                 print(\"-\"*70)\n",
    "#             print(\"\\n\")\n",
    "\n",
    "#         print(tag + \"Training complete!\")\n",
    "        \n",
    "    def evaluate(model, val_dataloader):\n",
    "        \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "        on our validation set.\n",
    "        \"\"\"\n",
    "        # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "        # the test time.\n",
    "        model.eval()\n",
    "\n",
    "        # Tracking variables\n",
    "        val_accuracy = []\n",
    "        val_loss = []\n",
    "\n",
    "        # For each batch in our validation set...\n",
    "        for batch in val_dataloader:\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Compute logits\n",
    "            with torch.no_grad():\n",
    "                logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(logits, b_labels.float())\n",
    "            val_loss.append(loss.item())\n",
    "            \n",
    "            accuracy = accuracy_thresh(logits.view(-1,l),b_labels.view(-1,l))\n",
    "        \n",
    "            val_accuracy.append(accuracy)\n",
    "\n",
    "        # Compute the average accuracy and loss over the validation set.\n",
    "        val_loss = np.mean(val_loss)\n",
    "        val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "        return val_loss, val_accuracy\n",
    "    \n",
    "    \n",
    "    def accuracy_thresh(y_pred, y_true, thresh:float=0.5, sigmoid:bool=True):\n",
    "        \"Compute accuracy when `y_pred` and `y_true` are the same size.\"\n",
    "        if sigmoid: \n",
    "            y_pred = y_pred.sigmoid()\n",
    "        return ((y_pred>thresh)==y_true.byte()).float().mean().item()\n",
    "\n",
    "    set_seed(42)    # Set seed for reproducibility\n",
    "    bert_classifier, optimizer, scheduler = initialize_model(epochs=int(param['options']['params']['max_epochs']))\n",
    "    train(bert_classifier, train_dataloader, val_dataloader, epochs=int(param['options']['params']['max_epochs']), evaluation=True)\n",
    "    \n",
    "\n",
    "#     tokenizer.save_pretrained(MODEL_DIRECTORY)\n",
    "#     print(tag + \"tokenizer saved in \" + MODEL_DIRECTORY)\n",
    "#     best_model.model.save_pretrained(MODEL_DIRECTORY)\n",
    "#     print(tag + \"model saved in \" + MODEL_DIRECTORY)\n",
    "\n",
    "    print(tag + \"Model fine-tuning successfully finished\")\n",
    "    returns = {}\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- process=fine_tuning_progress model=bert_classification_en max_epoch=2 -- tokenizer intialized\n",
      "-- process=fine_tuning_progress model=bert_classification_en max_epoch=2 -- Data vectorization: started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2323: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- process=fine_tuning_progress model=bert_classification_en max_epoch=2 -- Data vectorization: finished.\n",
      "-- process=fine_tuning_progress model=bert_classification_en max_epoch=2 -- #Training data: 90, #Test data: 10\n",
      "Start training...\n",
      "\n",
      "-- process=fine_tuning_progress model=bert_classification_en max_epoch=2 -- :   0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- process=fine_tuning_progress model=bert_classification_en max_epoch=2 -- :  96%|█████████▌| 22/23 [00:08<00:00,  2.57it/s, loss=0.544]   1    |   22    |   0.544313   |     -      |     -     |   8.96   \n",
      "-- process=fine_tuning_progress model=bert_classification_en max_epoch=2 -- : 100%|██████████| 23/23 [00:08<00:00,  2.57it/s, loss=0.544]\n",
      "----------------------------------------------------------------------\n",
      "-- process=fine_tuning_progress model=bert_classification_en max_epoch=2 -- tokenizer saved in /srv/app/model/data/classification/en/bert_classification_en_finetuned_test\n",
      "-- process=fine_tuning_progress model=bert_classification_en max_epoch=2 -- model saved in /srv/app/model/data/classification/en/bert_classification_en_finetuned_test\n",
      "-- process=fine_tuning_progress model=bert_classification_en max_epoch=2 -- [0/2] train loss: 0.5443, valid loss: 0.4220, valid accuracy: 0.9028 [12s]\n",
      "-- process=fine_tuning_progress model=bert_classification_en max_epoch=2 -- :  96%|█████████▌| 22/23 [00:08<00:00,  2.55it/s, loss=0.44]    2    |   22    |   0.440368   |     -      |     -     |   8.79   \n",
      "-- process=fine_tuning_progress model=bert_classification_en max_epoch=2 -- : 100%|██████████| 23/23 [00:08<00:00,  2.62it/s, loss=0.44]\n",
      "----------------------------------------------------------------------\n",
      "-- process=fine_tuning_progress model=bert_classification_en max_epoch=2 -- tokenizer saved in /srv/app/model/data/classification/en/bert_classification_en_finetuned_test\n",
      "-- process=fine_tuning_progress model=bert_classification_en max_epoch=2 -- model saved in /srv/app/model/data/classification/en/bert_classification_en_finetuned_test\n",
      "-- process=fine_tuning_progress model=bert_classification_en max_epoch=2 -- [1/2] train loss: 0.4404, valid loss: 0.3978, valid accuracy: 0.9028 [12s]\n",
      "-- process=fine_tuning_progress model=bert_classification_en max_epoch=2 -- Model fine-tuning successfully finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(model,df,param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4 - apply the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "name": "mltkc_apply"
   },
   "outputs": [],
   "source": [
    "def apply(model,df,param):\n",
    "    tag = \"-- process=fine_tuning_progress model={} max_epoch={} -- \".format(param['options']['params']['base_model'], param['options']['params']['max_epochs'])\n",
    "    predict_labels = list(df.columns)[1:]\n",
    "    l = len(predict_labels)\n",
    "    \n",
    "    MODEL_DIRECTORY = \"/srv/app/model/data/classification\"\n",
    "    MODEL_DIRECTORY = os.path.join(MODEL_DIRECTORY, param['options']['params']['lang'], param['options']['model_name'])\n",
    "    tokenizer = BertTokenizer.from_pretrained(MODEL_DIRECTORY)\n",
    "    MODEL_NAME = os.path.join(\"/srv/app/model/data/classification\", param['options']['params']['lang'], param['options']['params']['base_model'])\n",
    "    MODEL_DIRECTORY = os.path.join(\"/srv/app/model/data/classification\", param['options']['params']['lang'],param['options']['model_name'])\n",
    "    model = BertClassifier(MODEL_NAME,l)\n",
    "    model = model.to(device)\n",
    "    model.load_state_dict(torch.load(os.path.join(MODEL_DIRECTORY, \"pytorch_model.pt\")))\n",
    "    print(tag + \"Fine-tuned model reloaded.\")\n",
    "    model.eval()\n",
    "    \n",
    "    def text_preprocessing(text):\n",
    "        if param['options']['params']['lang'] == \"en\":\n",
    "            text = text.lower()\n",
    "            text = re.sub(r\"what's\", \"what is \", text)\n",
    "            text = re.sub(r\"won't\", \"will not \", text)\n",
    "            text = re.sub(r\"\\'s\", \" \", text)\n",
    "            text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "            text = re.sub(r\"can't\", \"can not \", text)\n",
    "            text = re.sub(r\"n't\", \" not \", text)\n",
    "            text = re.sub(r\"i'm\", \"i am \", text)\n",
    "            text = re.sub(r\"\\'re\", \" are \", text)\n",
    "            text = re.sub(r\"\\'d\", \" would \", text)\n",
    "            text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "            text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "            text = re.sub(r\"\\'\\n\", \" \", text)\n",
    "            text = re.sub(r\"-\", \" \", text)\n",
    "            text = re.sub(r\"\\'\\xa0\", \" \", text)\n",
    "            text = re.sub('\\s+', ' ', text)\n",
    "            text = ''.join(c for c in text if not c.isnumeric())\n",
    "            text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
    "            text = re.sub(r'&amp;', '&', text)\n",
    "            text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        else:\n",
    "            text = re.sub(r'[\\r\\t\\n\\u3000]', '', text)\n",
    "            text = text.lower()\n",
    "            text = text.strip()\n",
    "        return text\n",
    "\n",
    "    def preprocessing_for_bert(data):\n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "        for sent in data:\n",
    "            encoded_sent = tokenizer.encode_plus(\n",
    "            text = text_preprocessing(sent),   #preprocess sentence\n",
    "            add_special_tokens = True,         #Add `[CLS]` and `[SEP]`\n",
    "            max_length= max_length_src  ,             #Max length to truncate/pad\n",
    "            pad_to_max_length = True,          #pad sentence to max length \n",
    "            return_attention_mask= True        #Return attention mask \n",
    "            )\n",
    "            # Add the outputs to the lists\n",
    "            input_ids.append(encoded_sent.get('input_ids'))\n",
    "            attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "        \n",
    "        #convert lists to tensors\n",
    "        input_ids = torch.tensor(input_ids)\n",
    "        attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "        return input_ids,attention_masks\n",
    "    \n",
    "    X = df[param['feature_variables'][0]].values.tolist()\n",
    "    labels = list(df.columns)[1:]\n",
    "#     labels = [i + \" score\" for i in labels]\n",
    "#     X = df.text.values\n",
    "    y = df[labels].values\n",
    "#     X_train, X_val, y_train, y_val =train_test_split(X, y, test_size=0.0, random_state=42, shuffle=True)\n",
    "#     print(tag + \"Data vectorization: started\")\n",
    "    train_inputs, train_masks = preprocessing_for_bert(X)\n",
    "    train_labels = torch.tensor(y)\n",
    "#     val_inputs, val_masks = preprocessing_for_bert(X_val)\n",
    "#     val_labels = torch.tensor(y_val)\n",
    "    \n",
    "    # Create the DataLoader for our training set\n",
    "    train_data = TensorDataset(train_inputs,train_masks, train_labels)\n",
    "    train_dataloader = DataLoader(train_data, batch_size=batch_size_train, shuffle=False)\n",
    "    \n",
    "    all_logits = []\n",
    "    for batch in train_dataloader:\n",
    "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
    "#         train_inputs, train_masks = preprocessing_for_bert(X[i])\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "#         print(\"Process one batch input\")\n",
    "        all_logits.append(logits)\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "\n",
    "    probs = all_logits.sigmoid().cpu().numpy()\n",
    "    returns = pd.DataFrame(probs,columns=predict_labels)\n",
    "    print(tag + \"apply function successfully finished\")\n",
    "\n",
    "    return returns\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- process=fine_tuning_progress model=bert_classification_en max_epoch=2 -- Fine-tuned model reloaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2323: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process one batch input\n",
      "Process one batch input\n",
      "Process one batch input\n",
      "Process one batch input\n",
      "Process one batch input\n",
      "Process one batch input\n",
      "Process one batch input\n",
      "Process one batch input\n",
      "Process one batch input\n",
      "Process one batch input\n",
      "Process one batch input\n",
      "Process one batch input\n",
      "Process one batch input\n",
      "Process one batch input\n",
      "Process one batch input\n",
      "Process one batch input\n",
      "Process one batch input\n",
      "Process one batch input\n",
      "Process one batch input\n",
      "Process one batch input\n",
      "Process one batch input\n",
      "Process one batch input\n",
      "Process one batch input\n",
      "Process one batch input\n",
      "Process one batch input\n"
     ]
    }
   ],
   "source": [
    "returns = apply(None,df,param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 5 - save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "name": "mltkc_save"
   },
   "outputs": [],
   "source": [
    "# save model to name in expected convention \"<algo_name>_<model_name>.h5\"\n",
    "def save(model, name):\n",
    "    return {}\n",
    "#     model_dir_path = Path(path)\n",
    "#     tokenizer.save_pretrained(model_dir_path)\n",
    "#     print(\"tokenizer saved.\")\n",
    "#     best_model.model.save_pretrained(model_dir_path)\n",
    "#     print(\"model saved. Successfully finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 6 - load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "name": "mltkc_load"
   },
   "outputs": [],
   "source": [
    "# load model from name in expected convention \"<algo_name>_<model_name>.h5\"\n",
    "def load(path):\n",
    "    model = {}\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load(MODEL_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 7 - provide a summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "name": "mltkc_summary"
   },
   "outputs": [],
   "source": [
    "# return model summary\n",
    "def summary(model=None):\n",
    "    returns = {}\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Stages\n",
    "All subsequent cells are not tagged and can be used for further freeform code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_name(text):\n",
    "    dialog_a = param['options']['params']['dialog_a']\n",
    "    dialog_b = param['options']['params']['dialog_b']\n",
    "    text = text.replace(\"cu: \",dialog_a).replace(\"oper: \",dialog_b)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cust:how can I help you today? op:Hi I want to change my product'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_name(\"cu: how can I help you today? oper: Hi I want to change my product\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
