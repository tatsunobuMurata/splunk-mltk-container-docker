{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dialogue Summarization with transformer-based pre-trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains an example workflow how to work on custom containerized code that seamlessly interfaces with the Splunk Machine Learning Toolkit (MLTK) Container for TensorFlow. This script contains an example of how to conduct **End-to-End automatic summarization task over English dialogue transcripts** using the transformers library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 0 - import libraries\n",
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "name": "mltkc_import"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy path in runtime: /opt/conda/lib/python3.8/site-packages/numpy/__init__.py\n",
      "numpy version in runtime: 1.22.1\n"
     ]
    }
   ],
   "source": [
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",
    "# import sys\n",
    "# sys.path.insert(1, '/opt/conda/lib/python3.8/site-packages')\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "import os\n",
    "path = os.path.abspath(np.__file__)\n",
    "print(\"numpy path in runtime: \" + path)\n",
    "print(\"numpy version in runtime: \" + np.__version__)\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"/opt/conda/lib/python3.8/site-packages/samsum_tok\", local_files_only=True)\n",
    "# summarizer = AutoModelForSeq2SeqLM.from_pretrained(\"/opt/conda/lib/python3.8/site-packages/samsum\", local_files_only=True)\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"philschmid/bart-large-cnn-samsum\")\n",
    "# # tokenizer.save_pretrained(\"/opt/conda/lib/python3.8/site-packages/samsum_tok\")\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"philschmid/bart-large-cnn-samsum\")\n",
    "# model.save_pretrained(\"/opt/conda/lib/python3.8/site-packages/samsum\")\n",
    "\n",
    "MODEL_DIRECTORY = \"/srv/app/model/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.22.1\n",
      "pandas version: 1.4.3\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing purposes\n",
    "print(\"numpy version: \" + np.__version__)\n",
    "print(\"pandas version: \" + pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "# summarizer = pipeline(\"summarization\", model=\"lidiya/bart-base-samsum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1 - get a data sample from Splunk\n",
    "In Splunk run a search to pipe a prepared dataset into this environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| makeresults\n",
    "| eval text = \"Client: Hello Customer Service Representative: Welcome, Mr. Client, how are you today? Client: Welcome, I’m fine. Customer Service Representative: With you from the complaints and suggestions department, how can I help you today, Mr. Client? Client: I have a big problem with the product I bought from your site this week Customer Service Representative: Moments with me Mr. Client, I will review your request now Client: Ok Customer Service Representative: Your request number 100145 was on Wednesday correct Mr. Client?  Client: Yup Customer Service Representative: Ok, Mr. (client’s name) can you explain to me what is the problem exactly?  Client: I didn’t like the product and I didn’t get it as the same picture on the company website, you are liars! Customer Service Representative: I apologize very much to you Mr. (client’s name) but on our website, we put pictures of real products only, and the proof is the positive comments and evaluations from all our customers in Saudi Arabia Client: I don’t believe it; I believe my eyes who see the product Customer Service Representative: Sorry, Mr. (client’s name) I’m not accusing you of lying at all, and I will solve this problem immediately, can you please explain to me in detail how the product is different from the picture? Client: In the picture, the size of the product is bigger than what I received Customer Service Representative: Ok, Mr. Client if you opened our website, you would find the sizes written in detail in the product description, if you can look at it now while you are with me on the line? Client: Ok Customer Service Representative: Take your time, Mr. (client’s name) Client: it is Written but the product in the picture is very big than the size that is written in the description? Customer Service Representative: This is because of the angle of photography Mr. (client’s name) because we in the company must show all the details of the product to the customer, so the pictures are a little close to the product to show all the details to the customers, and we provide the product with a specific and clear description of materials and sizes and advantages and everything so that the customer knows the product that he will buy, are you still with me Mr. (client’s name)? Client: yes Customer Service Representative: Are there any other differences than the size Mr. (client’s name)?  Client: I don’t know how to use this product; I mean the way I used it and it didn’t work Customer Service Representative: There is no problem Mr. (client’s name) I will send you an accurate description of the method of use and will follow up with you one of the customer service representatives in the department of inquiries to clarify all the details and information about the product  Client: Pretty good! Customer Service Representative: Do you have any other complaints, Mr. Client? Client: I wanted to complain to the shipping representative who got me this product Customer Service Representative: Moments with me, Mr. Client now I will determine who is responsible for your shipping operation\"\n",
    "| fit MLTKContainer algo=transformers_dialogsum_en mode=stage epochs=100 text into app:transformers_dialogsum_model as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "name": "mltkc_stage"
   },
   "outputs": [],
   "source": [
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",
    "def stage(name):\n",
    "    with open(\"data/\"+name+\".csv\", 'r') as f:\n",
    "        df = pd.read_csv(f)\n",
    "    with open(\"data/\"+name+\".json\", 'r') as f:\n",
    "        param = json.load(f)\n",
    "    return df, param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0  Client: Hello Customer Service Representative:...\n",
      "(1, 1)\n",
      "{'options': {'params': {'algo': 'transformers_dialogsum2', 'mode': 'stage', 'epochs': '100'}, 'args': ['text'], 'feature_variables': ['text'], 'model_name': 'transformers_dialogsum_model', 'output_name': 'text', 'algo_name': 'MLTKContainer', 'mlspl_limits': {'handle_new_cat': 'default', 'max_distinct_cat_values': '100', 'max_distinct_cat_values_for_classifiers': '100', 'max_distinct_cat_values_for_scoring': '100', 'max_fit_time': '600', 'max_inputs': '100000', 'max_memory_usage_mb': '4000', 'max_model_size_mb': '30', 'max_score_time': '600', 'use_sampling': 'true'}, 'kfold_cv': None}, 'feature_variables': ['text']}\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing purposes\n",
    "df, param = stage(\"transformers_dialogsum_model\")\n",
    "print(df)\n",
    "print(df.shape)\n",
    "print(str(param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 - create and initialize a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "name": "mltkc_init"
   },
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "# params: data and parameters\n",
    "# returns the model object which will be used as a reference to call fit, apply and summary subsequently\n",
    "def init(df,param):\n",
    "    # Load English parser and text blob (for sentiment analysis)\n",
    "    print(\"debug: start model initialization\")\n",
    "    model = {}\n",
    "    model[\"tokenizer\"] = AutoTokenizer.from_pretrained(\"philschmid/bart-large-cnn-samsum\")\n",
    "    model[\"summarizer\"] = AutoModelForSeq2SeqLM.from_pretrained(\"philschmid/bart-large-cnn-samsum\")\n",
    "    # model[\"tokenizer\"] = AutoTokenizer.from_pretrained(\"/srv/app/model/data/samsum_tok\")\n",
    "    # model[\"summarizer\"] = AutoModelForSeq2SeqLM.from_pretrained(\"/srv/app/model/data/samsum\")\n",
    "    print(\"debug: model initialized\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug: start model initialization\n",
      "debug: model initialized\n"
     ]
    }
   ],
   "source": [
    "model = init(df,param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3 - fit the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for this algorithm the model is pre-trained and therefore this stage is a placeholder only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "name": "mltkc_fit"
   },
   "outputs": [],
   "source": [
    "# returns a fit info json object\n",
    "def fit(model,df,param):\n",
    "    returns = {}\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4 - apply the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "name": "mltkc_apply"
   },
   "outputs": [],
   "source": [
    "def apply(model,df,param):\n",
    "    \n",
    "    print(\"debug: start apply\")\n",
    "    \n",
    "    X = df[param['feature_variables']].values.tolist()\n",
    "    temp_data=list()\n",
    "    \n",
    "#     tokenizer = AutoTokenizer.from_pretrained(\"/srv/app/model/data/samsum_tok\")\n",
    "#     summarizer = AutoModelForSeq2SeqLM.from_pretrained(\"/srv/app/model/data/samsum\")\n",
    "    print(\"debug: read input\")\n",
    "    print(\"numpy version: \" + np.__version__)\n",
    "    for i in range(len(X)):\n",
    "        print(\"debug: start tokenizing\")\n",
    "        input_ids = model[\"tokenizer\"](\n",
    "            str(X[i]), return_tensors=\"pt\"\n",
    "        ).input_ids  # Batch size 1\n",
    "        print(\"debug: finish tok and start summarizing\")\n",
    "        outputs = model[\"summarizer\"].generate(input_ids, max_length=100)\n",
    "        print(\"debug: finish summarizing and start decoding\")\n",
    "        summary = model[\"tokenizer\"].decode(outputs[0], skip_special_tokens=True)\n",
    "        print(\"debug: finish decoding\")\n",
    "        temp_data.append(summary)\n",
    "        \n",
    "    column_names=[\"summaries\"]\n",
    "    returns=pd.DataFrame(temp_data, columns=column_names)\n",
    "    print(\"debug: finished\")\n",
    "        \n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug: start apply\n",
      "debug: read input\n",
      "numpy version: 1.22.1\n",
      "debug: start tokenizing\n",
      "debug: finish tok and start summarizing\n",
      "debug: finish summarizing and start decoding\n",
      "debug: finish decoding\n",
      "debug: finished\n",
      "Client has a problem with the product he bought from the company this week. The size of the product in the picture is bigger than the size that is written in the description. Customer Service Representative will send him an accurate description of the method of use and he will follow up with one of the customer service representatives in the department of inquiries to clarify all the details.\n"
     ]
    }
   ],
   "source": [
    "returns = apply(model,df,param)\n",
    "print(returns[\"summaries\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 5 - save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "name": "mltkc_save"
   },
   "outputs": [],
   "source": [
    "# save model to name in expected convention \"<algo_name>_<model_name>.h5\"\n",
    "def save(model,name):\n",
    "    # model will not be saved or reloaded as it is pre-built\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 6 - load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "name": "mltkc_load"
   },
   "outputs": [],
   "source": [
    "# load model from name in expected convention \"<algo_name>_<model_name>.h5\"\n",
    "def load(name):\n",
    "    # model will not be saved or reloaded as it is pre-built\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 7 - provide a summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "name": "mltkc_summary"
   },
   "outputs": [],
   "source": [
    "# return model summary\n",
    "def summary(model=None):\n",
    "    returns = {}\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Stages\n",
    "All subsequent cells are not tagged and can be used for further freeform code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
