{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning T5 Japanese model on KDDI paired data for dialogue summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script preprocesses data input from Splunk search and performs fine-tuning on T5 Japanese model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 0 - import libraries\n",
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "name": "mltkc_import"
   },
   "outputs": [],
   "source": [
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",
    "# import sys\n",
    "# sys.path.insert(1, '/opt/conda/lib/python3.8/site-packages')\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "# import neologdn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# parameters for finetuning\n",
    "MODEL_NAME = \"sonoisa/t5-base-japanese\"\n",
    "# MODEL_NAME = \"Huaibo/t5_dialog_jp\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "max_length_src = 400\n",
    "max_length_target = 300\n",
    "\n",
    "batch_size_train = 8\n",
    "batch_size_valid = 8\n",
    "\n",
    "epochs = 100\n",
    "patience = 20\n",
    "\n",
    "# path to save fine-tuned model\n",
    "MODEL_DIRECTORY = \"/srv/app/model/data/t5_finetune_jp\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.22.1\n",
      "pandas version: 1.4.3\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing purposes\n",
    "print(\"numpy version: \" + np.__version__)\n",
    "print(\"pandas version: \" + pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "# summarizer = pipeline(\"summarization\", model=\"lidiya/bart-base-samsum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1 - get a data sample from Splunk\n",
    "In Splunk run a search to pipe a prepared dataset into this environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| makeresults\n",
    "| eval text = \"お客様：こんにちは、；従業者：こんにちは、；お客様、ご機嫌いかがでしょうか？；お客様：こんにちは、元気です。私はwifiサービスの性能に大きな問題を抱えています。先月から御社のwifiサービスを使い始めたのですが、購入したサービスのようにうまく機能していないのです。；従業者：今、ユーザー番号を確認します。お客様のユーザー番号は100145で、先月に作成されたものですが、よろしいですか？；お客様：はい ；従業者：お客様さん、何が問題なのか説明してもらえますか？；お客様：はい。私はあなたの無線LANのプレミアムバージョンに加入しており、それは通常の加入よりも高速であるはずです。しかし、とても遅いのです。；従業者：しかし、私たちのセンターから提供する無線LANサービスは、あなたが契約したパッケージの説明とまったく同じです。；お客様：私は自分の目で見た無線LANの速度を信じています。；従業者：申し訳ございませんが、お客様、私はあなたが嘘をついていると非難するつもりは全くありません、私はすぐにこの問題を解決します、あなたは私に詳細に説明することができますか？無線LANがあなたの期待に比べてどのように遅いですか？；お客様様：無線LANは時々オフラインで、物をダウンロードするのに時間がかかります。；従業者：ローカルネットワークに何らかの問題があるかもしれませんね。ルーターをチェックしていただけますか？；お客様: はい。 ；従業者： ごゆっくり、お客様。；お客様：ルーターの電源は入っていますが、ランプが点滅しています ；従業者：ランプが点滅するのは正常ではありません。ランプは接続の安定性を示すものですから、ランプが点滅するのは正常なことではありません。どのランプが点滅しているのか、またその点滅の速さを知る必要があるかもしれませんが、ご確認いただけますか？；お客様：はい ；従業者：アクティブという名前の光はありますか？；お客様：はい。それは点滅しているものです。；従業者：それはあなたのネットワークの問題であるべきですあなたはすべてのケーブルがうまく接続されていることを確認することができますか？；お客様：はい ；従業者：無線LANを再接続しようとしたことがありますか？；お客様：はい、私はやったし、私は問題があなたの側にあると思う　；従業者：今私はあなたのローカルネットワークのデータ入力を確認します\"\n",
    "| fit MLTKContainer algo=transformers_dialogsum_jp mode=stage epochs=100 text into app:transformers_dialogjp_model as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "name": "mltkc_stage"
   },
   "outputs": [],
   "source": [
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",
    "def stage(name):\n",
    "    with open(\"data/\"+name+\".csv\", 'r') as f:\n",
    "        df = pd.read_csv(f)\n",
    "    with open(\"data/\"+name+\".json\", 'r') as f:\n",
    "        param = json.load(f)    \n",
    "    return df, param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  お客様：こんにちは、；従業者：こんにちは、；お客様、ご機嫌いかがでしょうか？；お客様：こんに...   \n",
      "1  お客様：こんにちは、；従業者：こんにちは、；お客様、ご機嫌いかがでしょうか？；お客様：こんに...   \n",
      "2                                                NaN   \n",
      "3  お客様：こんにちは、；従業者：こんにちは、；お客様、ご機嫌いかがでしょうか？；お客様：こんに...   \n",
      "4  お客様：こんにちは、；従業者：こんにちは、；お客様、ご機嫌いかがでしょうか？；お客様：こんに...   \n",
      "\n",
      "                                             summary  \n",
      "0  ▪対話経路: お客様 ▪対話者: 契約者本人 ▪RAS利用: なし ▪症状: WIFIが遅い...  \n",
      "1  ▪対話経路: お客様 ▪対話者: 契約者本人 ▪RAS利用: なし ▪症状: WIFIが遅い...  \n",
      "2  ▪対話経路: お客様 ▪対話者: 契約者本人 ▪RAS利用: なし ▪症状: WIFIが遅い...  \n",
      "3  ▪対話経路: お客様 ▪対話者: 契約者本人 ▪RAS利用: なし ▪症状: WIFIが遅い...  \n",
      "4                                                NaN  \n",
      "(5, 2)\n",
      "{'options': {'params': {'algo': 'transformers_dialogsum_jp', 'mode': 'stage', 'epochs': '100'}, 'args': ['text'], 'feature_variables': ['text'], 'model_name': 'transformers_dialogjp_model', 'output_name': 'text', 'algo_name': 'MLTKContainer', 'mlspl_limits': {'handle_new_cat': 'default', 'max_distinct_cat_values': '100', 'max_distinct_cat_values_for_classifiers': '100', 'max_distinct_cat_values_for_scoring': '100', 'max_fit_time': '600', 'max_inputs': '100000', 'max_memory_usage_mb': '4000', 'max_model_size_mb': '30', 'max_score_time': '600', 'use_sampling': 'true'}, 'kfold_cv': None}, 'feature_variables': ['text']}\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing purposes\n",
    "df, param = stage(\"kddi_data\")\n",
    "print(df.head())\n",
    "print(df.shape)\n",
    "print(str(param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 - create and initialize a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "name": "mltkc_init"
   },
   "outputs": [],
   "source": [
    "class T5FineTuner(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids, attention_mask=None, decoder_input_ids=None,\n",
    "        decoder_attention_mask=None, labels=None\n",
    "    ):\n",
    "        return self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "# initialize the model\n",
    "# params: data and parameters\n",
    "# returns the model object which will be used as a reference to call fit, apply and summary subsequently\n",
    "def init(df,param):\n",
    "    # Load English parser and text blob (for sentiment analysis)\n",
    "    print(\"debug: start model initialization\")\n",
    "    model = T5FineTuner()\n",
    "    model = model.to(device)\n",
    "    print(\"debug: model initialized\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug: start model initialization\n",
      "debug: model initialized\n"
     ]
    }
   ],
   "source": [
    "model = init(df,param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3 - fit the model\n",
    "\n",
    "We assume the input data frame has columns **text** and **summary**. The names can be changed later with rename(columns={'name': 'text'}\n",
    "As preprocessing of summary data, we convert the bullet point-structured data into natural language. Special delimiters: u'\\u25AA' and \":\". Make sure to check if Japanese colon is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "def make_summary(doc):\n",
    "    def new_dict():\n",
    "        dictionary = {\"対話経路\":'',\"対話者\":'',\"RAS利用\":'',\"症状\":'',\"発生頻度\": '',\"詳細\":'',\"本体SW\":'',\"通信環境\":'',\"周辺機器\":'',\"交換理由\":'',\"問診内容\":''}\n",
    "        return dictionary\n",
    "\n",
    "    def parcing(doc):\n",
    "        dictionary = new_dict()\n",
    "        separator = u'\\u25AA'\n",
    "        infos = doc.split(separator)\n",
    "        for info in infos:\n",
    "            l = info.split(\":\")\n",
    "            if len(l) == 2:\n",
    "                dictionary[l[0]] = l[-1].rstrip().lstrip()\n",
    "        return dictionary\n",
    "    def formating(dictionary):\n",
    "        line1 = dictionary[\"対話者\"]+\"である\"+dictionary[\"対話経路\"]+\"から\"+dictionary[\"症状\"]+\"という症状が\"+dictionary[\"発生頻度\"]+\"で発生するとの連絡です。\"\n",
    "        line2 = dictionary[\"詳細\"]+\" \"\n",
    "        line3 = \"\"\n",
    "        for key in [\"RAS利用\",\"本体SW\",\"通信環境\",\"周辺機器\",\"交換理由\",\"問診内容\"]:\n",
    "            if dictionary[key] != \"\":\n",
    "                line3 += (key + \"は\" + dictionary[key] + \"、\")\n",
    "        if line3 != \"\":\n",
    "            line3 = line3[:-1] + \"です\"\n",
    "\n",
    "        return line1 + line2 + line3\n",
    "    return formating(parcing(doc))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[\\r\\t\\n\\u3000]', '', text)\n",
    "#     text = neologdn.normalize(text)\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "data = df.query('text.notnull()', engine='python').query('summary.notnull()', engine='python')\n",
    "\n",
    "data = data.assign(\n",
    "    text=lambda x: x.text.map(lambda y: preprocess_text(y)),\n",
    "    summary=lambda x: x.summary.map(lambda y: make_summary(preprocess_text(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>お客様：こんにちは、；従業者：こんにちは、；お客様、ご機嫌いかがでしょうか？；お客様：こんに...</td>\n",
       "      <td>契約者本人であるお客様からwifiが遅いという症状が初めてで発生するとの連絡です。プレミアム...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>お客様：こんにちは、；従業者：こんにちは、；お客様、ご機嫌いかがでしょうか？；お客様：こんに...</td>\n",
       "      <td>契約者本人であるお客様からwifiが遅いという症状が初めてで発生するとの連絡です。プレミアム...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>お客様：こんにちは、；従業者：こんにちは、；お客様、ご機嫌いかがでしょうか？；お客様：こんに...</td>\n",
       "      <td>契約者本人であるお客様からwifiが遅いという症状が初めてで発生するとの連絡です。プレミアム...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  お客様：こんにちは、；従業者：こんにちは、；お客様、ご機嫌いかがでしょうか？；お客様：こんに...   \n",
       "1  お客様：こんにちは、；従業者：こんにちは、；お客様、ご機嫌いかがでしょうか？；お客様：こんに...   \n",
       "3  お客様：こんにちは、；従業者：こんにちは、；お客様、ご機嫌いかがでしょうか？；お客様：こんに...   \n",
       "\n",
       "                                             summary  \n",
       "0  契約者本人であるお客様からwifiが遅いという症状が初めてで発生するとの連絡です。プレミアム...  \n",
       "1  契約者本人であるお客様からwifiが遅いという症状が初めてで発生するとの連絡です。プレミアム...  \n",
       "3  契約者本人であるお客様からwifiが遅いという症状が初めてで発生するとの連絡です。プレミアム...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer initialized.\n",
      "data vectorization finished. Training data: 2, Test data: 1\n"
     ]
    }
   ],
   "source": [
    "# Data conversion\n",
    "def convert_batch_data(train_data, valid_data, tokenizer):\n",
    "\n",
    "    def generate_batch(data):\n",
    "\n",
    "        batch_src, batch_tgt = [], []\n",
    "        for src, tgt in data:\n",
    "            batch_src.append(src)\n",
    "            batch_tgt.append(tgt)\n",
    "\n",
    "        batch_src = tokenizer(\n",
    "            batch_src, max_length=max_length_src, truncation=True, padding=\"max_length\", return_tensors=\"pt\"\n",
    "        )\n",
    "        batch_tgt = tokenizer(\n",
    "            batch_tgt, max_length=max_length_target, truncation=True, padding=\"max_length\", return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return batch_src, batch_tgt\n",
    "\n",
    "    train_iter = DataLoader(train_data, batch_size=batch_size_train, shuffle=True, collate_fn=generate_batch)\n",
    "    valid_iter = DataLoader(valid_data, batch_size=batch_size_valid, shuffle=True, collate_fn=generate_batch)\n",
    "\n",
    "    return train_iter, valid_iter\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME, is_fast=True)\n",
    "\n",
    "print(\"Tokenizer initialized.\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data['text'], data['summary'], test_size=0.15, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "train_data = [(src, tgt) for src, tgt in zip(X_train, y_train)]\n",
    "valid_data = [(src, tgt) for src, tgt in zip(X_test, y_test)]\n",
    "\n",
    "train_iter, valid_iter = convert_batch_data(train_data, valid_data, tokenizer)\n",
    "\n",
    "print(\"data vectorization finished. Training data: \" + str(len(train_data)) + \", Test data: \" + str(len(valid_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(model, data, optimizer, PAD_IDX):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loop = 1\n",
    "    losses = 0\n",
    "    pbar = tqdm(data)\n",
    "    for src, tgt in pbar:\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        labels = tgt['input_ids'].to(device)\n",
    "        labels[labels[:, :] == PAD_IDX] = -100\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=src['input_ids'].to(device),\n",
    "            attention_mask=src['attention_mask'].to(device),\n",
    "            decoder_attention_mask=tgt['attention_mask'].to(device),\n",
    "            labels=labels\n",
    "        )\n",
    "        loss = outputs['loss']\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "        \n",
    "        pbar.set_postfix(loss=losses / loop)\n",
    "        loop += 1\n",
    "        \n",
    "    return losses / len(data)\n",
    "\n",
    "# Loss function\n",
    "def evaluate(model, data, PAD_IDX):\n",
    "    \n",
    "    model.eval()\n",
    "    losses = 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in data:\n",
    "\n",
    "            labels = tgt['input_ids'].to(device)\n",
    "            labels[labels[:, :] == PAD_IDX] = -100\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=src['input_ids'].to(device),\n",
    "                attention_mask=src['attention_mask'].to(device),\n",
    "                decoder_attention_mask=tgt['attention_mask'].to(device),\n",
    "                labels=labels\n",
    "            )\n",
    "            loss = outputs['loss']\n",
    "            losses += loss.item()\n",
    "        \n",
    "    return losses / len(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "name": "mltkc_fit"
   },
   "outputs": [],
   "source": [
    "# returns a fit info json object\n",
    "def fit(model,df,param):\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    PAD_IDX = tokenizer.pad_token_id\n",
    "    best_loss = float('Inf')\n",
    "    best_model = None\n",
    "    counter = 1\n",
    "\n",
    "    print(\"Start training.\")\n",
    "\n",
    "    for loop in range(1, epochs + 1):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        loss_train = train(model=model, data=train_iter, optimizer=optimizer, PAD_IDX=PAD_IDX)\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        loss_valid = evaluate(model=model, data=valid_iter, PAD_IDX=PAD_IDX)\n",
    "\n",
    "        print('[{}/{}] train loss: {:.4f}, valid loss: {:.4f} [{}{:.0f}s] counter: {} {}'.format(\n",
    "            loop, epochs, loss_train, loss_valid,\n",
    "            str(int(math.floor(elapsed_time / 60))) + 'm' if math.floor(elapsed_time / 60) > 0 else '',\n",
    "            elapsed_time % 60,\n",
    "            counter,\n",
    "            '**' if best_loss > loss_valid else ''\n",
    "        ))\n",
    "\n",
    "        if best_loss > loss_valid:\n",
    "            best_loss = loss_valid\n",
    "            best_model = copy.deepcopy(model)\n",
    "            counter = 1\n",
    "        else:\n",
    "            if counter > patience:\n",
    "                break\n",
    "\n",
    "            counter += 1\n",
    "\n",
    "    print(\"finished training.\")\n",
    "    returns = {}\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4 - apply the model\n",
    "\n",
    "This stage should be called after Stage 5 (saving model) and 6 (loading model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "name": "mltkc_apply"
   },
   "outputs": [],
   "source": [
    "def apply(model,df,param):\n",
    "    \n",
    "    def generate_text_from_model(text, trained_model, tokenizer, num_return_sequences=1):\n",
    "\n",
    "        trained_model.eval()\n",
    "\n",
    "        text = preprocess_text(text)\n",
    "        batch = tokenizer(\n",
    "            [text], max_length=max_length_src, truncation=True, padding=\"longest\", return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        outputs = trained_model.generate(\n",
    "            input_ids=batch['input_ids'].to(device),\n",
    "            attention_mask=batch['attention_mask'].to(device),\n",
    "            max_length=max_length_target,\n",
    "            repetition_penalty=8.0,  \n",
    "            num_beams=10,\n",
    "            num_return_sequences=num_return_sequences,  \n",
    "        )\n",
    "\n",
    "        generated_texts = [\n",
    "            tokenizer.decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=False) for ids in outputs\n",
    "        ]\n",
    "\n",
    "        return generated_texts\n",
    "\n",
    "    index = 100\n",
    "    body = valid_data[index][0]\n",
    "    summaries = valid_data[index][1]\n",
    "    generated_texts = generate_text_from_model(\n",
    "        text=body, trained_model=model[\"summarizer\"], tokenizer=model[\"tokenizer\"], num_return_sequences=1\n",
    "    )\n",
    "    print('Summarization')\n",
    "    print('\\n'.join(generated_texts[0].split('。')))\n",
    "    print()\n",
    "    print('ground truth')\n",
    "    print('\\n'.join(summaries.split('。')))\n",
    "    print()\n",
    "    print('original text')\n",
    "    print(body)\n",
    "        \n",
    "    return generated_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug: start apply\n",
      "debug: read input\n",
      "numpy version: 1.22.1\n",
      "debug: start tokenizing\n",
      "debug: finish tok and start summarizing\n",
      "debug: finish summarizing and start decoding\n",
      "debug: finish decoding\n",
      "debug: finished\n",
      "無線のプレミアムバージョンに加入しているユーザー番号は100145。通常加入よりも高速であるはずなのに、とても遅いという。お客様が契約したパッケージの説明とまったく同じだそう\n"
     ]
    }
   ],
   "source": [
    "returns = apply(model,df,param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 5 - save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "name": "mltkc_save"
   },
   "outputs": [],
   "source": [
    "# save model to name in expected convention \"<algo_name>_<model_name>.h5\"\n",
    "def save(path):\n",
    "    model_dir_path = Path(path)\n",
    "    tokenizer.save_pretrained(model_dir_path)\n",
    "    print(\"tokenizer saved.\")\n",
    "    best_model.model.save_pretrained(model_dir_path)\n",
    "    print(\"model saved. Successfully finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(MODEL_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 6 - load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "name": "mltkc_load"
   },
   "outputs": [],
   "source": [
    "# load model from name in expected convention \"<algo_name>_<model_name>.h5\"\n",
    "def load(path):\n",
    "    model = {}\n",
    "    model[\"tokenizer\"] = T5Tokenizer.from_pretrained(path)\n",
    "    model[\"summarizer\"] = T5ForConditionalGeneration.from_pretrained(path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load(MODEL_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 7 - provide a summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "name": "mltkc_summary"
   },
   "outputs": [],
   "source": [
    "# return model summary\n",
    "def summary(model=None):\n",
    "    returns = {}\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Stages\n",
    "All subsequent cells are not tagged and can be used for further freeform code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
