{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization fine-tuning script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script contains the source code of how to fine-tune a T5 model on **End-to-End automatic summarization task** in both English and Japanese.\n",
    "\n",
    "## Sample usage of SPL for fine-tuning:\n",
    "\n",
    "| inputlookup summarization_jp\n",
    "| fields body_text summary_text \n",
    "| rename summary_text as summary body_text as text\n",
    "| head 10\n",
    "| fit MLTKContainer algo=appNLP_summarization_final max_epochs=1 lang=jp base_model=t5_summarization_jp batch_size=4 summary from text into app:t5_summarization_jp_finetuned_final as extracted_summary\n",
    "\n",
    "## Sample usage of SPL for applying:\n",
    "\n",
    "| inputlookup summarization_jp\n",
    "| fields body_text summary_text \n",
    "| rename summary_text as summary body_text as text\n",
    "| head 10\n",
    "| apply t5_summarization_jp_finetuned_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 0 - import libraries\n",
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "name": "mltkc_import"
   },
   "outputs": [],
   "source": [
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "# from torchmetrics.text.rouge import ROUGEScore\n",
    "# tensorboard related\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tensorboard\n",
    "import datetime\n",
    "import logging\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "import psutil\n",
    "import shutil\n",
    "# Fine-tune parameters initialization\n",
    "MODEL_NAME = \"/srv/app/model/data\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "max_length_src = 400\n",
    "max_length_target = 200\n",
    "\n",
    "batch_size_train = 4\n",
    "batch_size_valid = 4\n",
    "\n",
    "epochs = 100\n",
    "patience = 20\n",
    "\n",
    "MODEL_DIRECTORY = \"/\"\n",
    "\n",
    "class T5FineTuner(nn.Module):\n",
    "    \n",
    "    def __init__(self, MODEL_NAME):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, local_files_only=True)\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids, attention_mask=None, decoder_input_ids=None,\n",
    "        decoder_attention_mask=None, labels=None\n",
    "    ):\n",
    "        return self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            labels=labels\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1 - get a data sample from Splunk\n",
    "In Splunk run a search to pipe a prepared dataset into this environment. (internal testing only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| inputlookup summarization_en\n",
    "| fields text summary\n",
    "| head 5\n",
    "| fit MLTKContainer algo=appNLP_summarization_final mode=stage max_epochs=1 lang=en base_model=t5_summarization_en summary from text into app:t5_summarization_en_finetuned_final as extracted_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "name": "mltkc_stage"
   },
   "outputs": [],
   "source": [
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",
    "def stage(name):\n",
    "    print(\"DEBUG stage call\")\n",
    "    print(\"DEBUG \" + name)\n",
    "    with open(\"/srv/notebooks/data/\"+name+\".csv\", 'r') as f:\n",
    "        df = pd.read_csv(f)\n",
    "    with open(\"/srv/notebooks/data/\"+name+\".json\", 'r') as f:\n",
    "        param = json.load(f) \n",
    "    return df, param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, param = stage(\"t5_summarization_jp_finetuned_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 - create and initialize a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "name": "mltkc_init"
   },
   "outputs": [],
   "source": [
    "def init(df,param):\n",
    "    tag = \"-- process=fine_tuning_progress model={} max_epoch={} -- \".format(param['options']['params']['base_model'], param['options']['params']['max_epochs'])\n",
    "\n",
    "    print(tag + \"Training data loaded with shape: \" + str(df.shape))\n",
    "    print(tag + \"Input parameters: \", param['options']['params'])\n",
    "    print(tag + \"Epoch number: \" + param['options']['params']['max_epochs'])\n",
    "    print(tag + \"Base model: \" + param['options']['params']['base_model'])\n",
    "    \n",
    "    print(tag + \"Model Initialization: started\")\n",
    "    MODEL_NAME = \"/srv/app/model/data/summarization\"\n",
    "    MODEL_NAME = os.path.join(MODEL_NAME, param['options']['params']['lang'], param['options']['params']['base_model'])\n",
    "    print(tag + \"Model file in \" + MODEL_NAME)\n",
    "    model = T5FineTuner(MODEL_NAME)\n",
    "    model = model.to(device)\n",
    "    print(tag + \"Model Initialization: successfully finished\")\n",
    "    # GPU memory calculation\n",
    "    t = torch.cuda.get_device_properties(0).total_memory\n",
    "    r = torch.cuda.memory_reserved(0)\n",
    "    a = torch.cuda.memory_allocated(0)\n",
    "    f = r-a  # free inside reserved\n",
    "    load1, load5, load15 = psutil.getloadavg()\n",
    "    cpu_usage = (load15/os.cpu_count()) * 100\n",
    "    stat = shutil.disk_usage(\"/\")\n",
    "    \n",
    "    print(tag + \"#GPU memory --Total memory: {}, --Memory reserved: {}, --Memory allocated: {}. #CPU: {}% occupied. #disk {}\".format(t,r,a,cpu_usage,stat))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- process=fine_tuning_progress model=t5_summarization_jp max_epoch=1 -- Training data loaded with shape: (10, 2)\n",
      "-- process=fine_tuning_progress model=t5_summarization_jp max_epoch=1 -- Input parameters:  {'algo': 'appNLP_summarization_final', 'mode': 'stage', 'max_epochs': '1', 'lang': 'jp', 'base_model': 't5_summarization_jp', 'batch_size': '4'}\n",
      "-- process=fine_tuning_progress model=t5_summarization_jp max_epoch=1 -- Epoch number: 1\n",
      "-- process=fine_tuning_progress model=t5_summarization_jp max_epoch=1 -- Base model: t5_summarization_jp\n",
      "-- process=fine_tuning_progress model=t5_summarization_jp max_epoch=1 -- Model Initialization: started\n",
      "-- process=fine_tuning_progress model=t5_summarization_jp max_epoch=1 -- Model file in /srv/app/model/data/summarization/jp/t5_summarization_jp\n",
      "-- process=fine_tuning_progress model=t5_summarization_jp max_epoch=1 -- Model Initialization: successfully finished\n",
      "-- process=fine_tuning_progress model=t5_summarization_jp max_epoch=1 -- #GPU memory --Total memory: 15634661376, --Memory reserved: 983564288, --Memory allocated: 891614208. #CPU: 0.5% occupied. #disk usage(total=156052275200, used=154126524416, free=1908973568)\n"
     ]
    }
   ],
   "source": [
    "model = init(df,param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3 - fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "name": "mltkc_fit"
   },
   "outputs": [],
   "source": [
    "def fit(model,df,param):  \n",
    "    tag = \"-- process=fine_tuning_progress model={} max_epoch={} -- \".format(param['options']['params']['base_model'], param['options']['params']['max_epochs'])\n",
    "    if \"batch_size\" in param['options']['params']:\n",
    "        print(tag + \"setting batch size to \", param['options']['params']['batch_size'])\n",
    "        batch_size_train = int(param['options']['params']['batch_size'])\n",
    "        batch_size_valid = int(param['options']['params']['batch_size'])\n",
    "\n",
    "    def preprocess_text(text):\n",
    "        text = re.sub(r'[\\r\\t\\n\\u3000]', '', text)\n",
    "        text = text.lower()\n",
    "        text = text.strip()\n",
    "        return text\n",
    "\n",
    "    data = df.query('text.notnull()', engine='python').query('summary.notnull()', engine='python')\n",
    "    data = data.assign(\n",
    "        text=lambda x: x.text.map(lambda y: preprocess_text(y)),\n",
    "        summary=lambda x: x.summary.map(lambda y: preprocess_text(y)))\n",
    "    # Data conversion\n",
    "    def convert_batch_data(train_data, valid_data, tokenizer):\n",
    "\n",
    "        def generate_batch(data):\n",
    "\n",
    "            batch_src, batch_tgt = [], []\n",
    "            for src, tgt in data:\n",
    "                batch_src.append(src)\n",
    "                batch_tgt.append(tgt)\n",
    "\n",
    "            batch_src = tokenizer(\n",
    "                batch_src, max_length=max_length_src, truncation=True, padding=\"max_length\", return_tensors=\"pt\"\n",
    "            )\n",
    "            batch_tgt = tokenizer(\n",
    "                batch_tgt, max_length=max_length_target, truncation=True, padding=\"max_length\", return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            return batch_src, batch_tgt\n",
    "\n",
    "        train_iter = DataLoader(train_data, batch_size=batch_size_train, shuffle=True, collate_fn=generate_batch)\n",
    "        valid_iter = DataLoader(valid_data, batch_size=batch_size_valid, shuffle=True, collate_fn=generate_batch)\n",
    "\n",
    "        return train_iter, valid_iter\n",
    "    MODEL_NAME = \"/srv/app/model/data/summarization\"\n",
    "    MODEL_NAME = os.path.join(MODEL_NAME, param['options']['params']['lang'], param['options']['params']['base_model'])\n",
    "    tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME, is_fast=True)\n",
    "    print(tag + \"tokenizer intialized\")\n",
    "    print(tag + \"Data vectorization: started\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        data['text'], data['summary'], test_size=0.15, random_state=42, shuffle=True\n",
    "    )\n",
    "\n",
    "    train_data = [(src, tgt) for src, tgt in zip(X_train, y_train)]\n",
    "    valid_data = [(src, tgt) for src, tgt in zip(X_test, y_test)]\n",
    "\n",
    "    train_iter, valid_iter = convert_batch_data(train_data, valid_data, tokenizer)\n",
    "    print(tag + \"Data vectorization: finished.\")\n",
    "    print(tag + \"#Training data: \" + str(len(train_data)) + \", #Test data: \" + str(len(valid_data)))\n",
    "\n",
    "    # Training function\n",
    "    def train(model, data, optimizer, PAD_IDX, i):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        loop = 1\n",
    "        total = len(data)\n",
    "        losses = 0\n",
    "        for src, tgt in data:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            labels = tgt['input_ids'].to(device)\n",
    "            labels[labels[:, :] == PAD_IDX] = -100\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=src['input_ids'].to(device),\n",
    "                attention_mask=src['attention_mask'].to(device),\n",
    "                decoder_attention_mask=tgt['attention_mask'].to(device),\n",
    "                labels=labels\n",
    "            )\n",
    "            loss = outputs['loss']\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses += loss.item()\n",
    "\n",
    "            print(tag + \"Processed {}% of the {}-th epoch. Finished {} out of {} batches. Loss: {} \".format(round(loop/total*100), i, loop, total, round(losses / loop,2)), flush=True)\n",
    "            loop += 1\n",
    "\n",
    "        return losses / len(data)\n",
    "\n",
    "    # Loss function\n",
    "    def evaluate(model, data, PAD_IDX):\n",
    "\n",
    "        model.eval()\n",
    "        losses = 0\n",
    "        with torch.no_grad():\n",
    "            for src, tgt in data:\n",
    "\n",
    "                labels = tgt['input_ids'].to(device)\n",
    "                labels[labels[:, :] == PAD_IDX] = -100\n",
    "\n",
    "                outputs = model(\n",
    "                    input_ids=src['input_ids'].to(device),\n",
    "                    attention_mask=src['attention_mask'].to(device),\n",
    "                    decoder_attention_mask=tgt['attention_mask'].to(device),\n",
    "                    labels=labels\n",
    "                )\n",
    "                loss = outputs['loss']\n",
    "                losses += loss.item()\n",
    "\n",
    "        return losses / len(data)\n",
    "\n",
    "    epochs = int(param['options']['params']['max_epochs'])\n",
    "    MODEL_DIRECTORY = \"/srv/app/model/data/summarization\"\n",
    "    MODEL_DIRECTORY = os.path.join(MODEL_DIRECTORY, param['options']['params']['lang'], param['options']['model_name'])\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    PAD_IDX = tokenizer.pad_token_id\n",
    "    best_loss = float('Inf')\n",
    "    best_model = None\n",
    "    counter = 1\n",
    "\n",
    "    print(tag + 'Model fine-tuning started with {} epochs'.format(epochs))\n",
    "\n",
    "    for loop in range(1, epochs + 1):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        loss_train = train(model=model, data=train_iter, optimizer=optimizer, PAD_IDX=PAD_IDX, i=loop)\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        loss_valid = evaluate(model=model, data=valid_iter, PAD_IDX=PAD_IDX)\n",
    "        \n",
    "        t = torch.cuda.get_device_properties(0).total_memory\n",
    "        r = torch.cuda.memory_reserved(0)\n",
    "        a = torch.cuda.memory_allocated(0)\n",
    "        f = r-a  # free inside reserved\n",
    "        load1, load5, load15 = psutil.getloadavg()\n",
    "        cpu_usage = (load15/os.cpu_count()) * 100\n",
    "        stat = shutil.disk_usage(\"/\")\n",
    "        print(tag + \"#GPU memory --Total memory: {}, --Memory reserved: {}, --Memory allocated: {}. #CPU: {}% occupied. #disk {}\".format(t,r,a,cpu_usage,stat), flush=True)\n",
    "\n",
    "        print(tag + '[{}/{}] train loss: {:.4f}, valid loss: {:.4f} [{}{:.0f}s] counter: {} {}'.format(\n",
    "            loop, epochs, loss_train, loss_valid,\n",
    "            str(int(math.floor(elapsed_time / 60))) + 'm' if math.floor(elapsed_time / 60) > 0 else '',\n",
    "            elapsed_time % 60,\n",
    "            counter,\n",
    "            '**' if best_loss > loss_valid else ''\n",
    "        ),flush=True)\n",
    "\n",
    "        if best_loss > loss_valid:\n",
    "            best_loss = loss_valid\n",
    "            best_model = copy.deepcopy(model)\n",
    "            counter = 1\n",
    "        else:\n",
    "            if counter > patience:\n",
    "                break\n",
    "\n",
    "            counter += 1\n",
    "\n",
    "        tokenizer.save_pretrained(MODEL_DIRECTORY)\n",
    "        print(tag + \"tokenizer saved in \" + MODEL_DIRECTORY, flush=True)\n",
    "        best_model.model.save_pretrained(MODEL_DIRECTORY)\n",
    "        print(tag + \"model saved in \" + MODEL_DIRECTORY, flush=True)\n",
    "\n",
    "    print(tag + \"Model fine-tuning successfully finished\")\n",
    "    returns = {}\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model,df,param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4 - apply the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "name": "mltkc_apply"
   },
   "outputs": [],
   "source": [
    "def apply(model,df,param):\n",
    "    print(\"DEBUG: enter apply\")\n",
    "    print(param)\n",
    "    tag = \"-- process=fine_tuning_progress model={} max_epoch={} -- \".format(param['options']['params']['base_model'], param['options']['params']['max_epochs'])\n",
    "    MODEL_DIRECTORY = \"/srv/app/model/data/summarization\"\n",
    "    MODEL_DIRECTORY = os.path.join(MODEL_DIRECTORY, param['options']['params']['lang'], param['options']['model_name'])\n",
    "    model = {}\n",
    "    print(MODEL_DIRECTORY)\n",
    "    model[\"tokenizer\"] = T5Tokenizer.from_pretrained(MODEL_DIRECTORY)\n",
    "    model[\"summarizer\"] = T5ForConditionalGeneration.from_pretrained(MODEL_DIRECTORY)\n",
    "    print(\"DEBUG: model inited\")\n",
    "    X = df[param['feature_variables'][0]].values.tolist()\n",
    "    temp_data=list()\n",
    "    print(tag + \"apply function read inputs\")\n",
    "    for i in range(len(X)):\n",
    "        batch = model[\"tokenizer\"](str(X[i]), max_length=400, truncation=True, return_tensors=\"pt\")\n",
    "        outputs = model[\"summarizer\"].generate(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], max_length=400,repetition_penalty=8.0,num_beams=15)\n",
    "        summary = [model[\"tokenizer\"].decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=False) for ids in outputs]\n",
    "        temp_data += summary\n",
    "    cols={\"summary\": temp_data}\n",
    "    returns=pd.DataFrame(data=cols)\n",
    "    print(tag + \"apply function successfully finished\")\n",
    "        \n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = apply(model,df,param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 5 - save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "name": "mltkc_save"
   },
   "outputs": [],
   "source": [
    "# save model to name in expected convention \"<algo_name>_<model_name>.h5\"\n",
    "def save(model, name):\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 6 - load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "name": "mltkc_load"
   },
   "outputs": [],
   "source": [
    "# load model from name in expected convention \"<algo_name>_<model_name>.h5\"\n",
    "def load(path):\n",
    "    print(\"DEBUG: load\")\n",
    "    model = {}\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 7 - provide a summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "name": "mltkc_summary"
   },
   "outputs": [],
   "source": [
    "# return model summary\n",
    "def summary(model=None):\n",
    "    returns = {}\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Stages\n",
    "All subsequent cells are not tagged and can be used for further freeform code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
