{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization fine-tuning script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script contains the source code of how to fine-tune a t5 model on **End-to-End automatic summarization task** in both English and Japanese."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 0 - import libraries\n",
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "name": "mltkc_import"
   },
   "outputs": [],
   "source": [
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",
    "# import sys\n",
    "# sys.path.insert(1, '/opt/conda/lib/python3.8/site-packages')\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "# import neologdn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "# from torchmetrics.text.rouge import ROUGEScore\n",
    "# tensorboard related\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tensorboard\n",
    "import datetime\n",
    "import logging\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "import psutil\n",
    "import shutil\n",
    "# Fine-tune parameters initialization\n",
    "MODEL_NAME = \"/srv/app/model/data\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "max_length_src = 400\n",
    "max_length_target = 200\n",
    "\n",
    "batch_size_train = 4\n",
    "batch_size_valid = 4\n",
    "\n",
    "epochs = 100\n",
    "patience = 20\n",
    "\n",
    "MODEL_DIRECTORY = \"/\"\n",
    "\n",
    "class T5FineTuner(nn.Module):\n",
    "    \n",
    "    def __init__(self, MODEL_NAME):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids, attention_mask=None, decoder_input_ids=None,\n",
    "        decoder_attention_mask=None, labels=None\n",
    "    ):\n",
    "        return self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "    \n",
    "class BartFineTuner(nn.Module):\n",
    "    \n",
    "    def __init__(self, MODEL_NAME):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids, attention_mask=None, decoder_input_ids=None,\n",
    "        decoder_attention_mask=None, labels=None\n",
    "    ):\n",
    "        return self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            labels=labels\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1 - get a data sample from Splunk\n",
    "In Splunk run a search to pipe a prepared dataset into this environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| inputlookup summarization_en\n",
    "| fields text summary\n",
    "| head 5\n",
    "| fit MLTKContainer algo=appNLP_summarization_test max_epochs=1 lang=en base_model=t5_summarization_en metrics=rouge1_fmeasure type=dialog dialog_a=cust: dialog_b=op: summary from text into app:t5_summarization_en_finetuned_test as extracted_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "name": "mltkc_stage"
   },
   "outputs": [],
   "source": [
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",
    "def stage(name):\n",
    "    print(\"DEBUG stage call\")\n",
    "    print(\"DEBUG\" + name)\n",
    "    with open(\"/srv/notebooks/data/\"+name+\".csv\", 'r') as f:\n",
    "        df = pd.read_csv(f)\n",
    "    with open(\"/srv/notebooks/data/\"+name+\".json\", 'r') as f:\n",
    "        param = json.load(f) \n",
    "#         param = {}\n",
    "    return df, param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG stage call\n",
      "DEBUGt5_summarization_en_finetuned_final\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'options': {'params': {'algo': 'appNLP_summarization_final',\n",
       "   'max_epochs': '10',\n",
       "   'mode': 'stage',\n",
       "   'lang': 'en',\n",
       "   'base_model': 't5_summarization_en',\n",
       "   'batch_size': '4'},\n",
       "  'args': ['summary', 'text'],\n",
       "  'target_variable': ['summary'],\n",
       "  'feature_variables': ['text'],\n",
       "  'model_name': 't5_summarization_en_finetuned_final',\n",
       "  'output_name': 'extracted_summary',\n",
       "  'algo_name': 'MLTKContainer',\n",
       "  'mlspl_limits': {'handle_new_cat': 'default',\n",
       "   'max_distinct_cat_values': '100',\n",
       "   'max_distinct_cat_values_for_classifiers': '100',\n",
       "   'max_distinct_cat_values_for_scoring': '100',\n",
       "   'max_fit_time': '600',\n",
       "   'max_inputs': '100000',\n",
       "   'max_memory_usage_mb': '4000',\n",
       "   'max_model_size_mb': '30',\n",
       "   'max_score_time': '600',\n",
       "   'use_sampling': 'true'},\n",
       "  'kfold_cv': None},\n",
       " 'feature_variables': ['text'],\n",
       " 'target_variables': ['summary']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df, param = stage(\"t5_summarization_en_finetuned_final\")\n",
    "param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 - create and initialize a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "name": "mltkc_init"
   },
   "outputs": [],
   "source": [
    "def init(df,param):\n",
    "    tag = \"-- process=fine_tuning_progress model={} max_epoch={} -- \".format(param['options']['params']['base_model'], param['options']['params']['max_epochs'])\n",
    "#     df, param = df, param\n",
    "#     if df is not None:\n",
    "#         print(\"received df\")\n",
    "#     if param is not None:\n",
    "#         print(\"received param\")\n",
    "#         print(param)\n",
    "#     print(\"DEBUG init call\")\n",
    "\n",
    "    print(tag + \"Training data loaded with shape: \" + str(df.shape))\n",
    "    print(tag + \"Input parameters: \", param['options']['params'])\n",
    "    print(tag + \"Epoch number: \" + param['options']['params']['max_epochs'])\n",
    "    print(tag + \"Base model: \" + param['options']['params']['base_model'])\n",
    "    \n",
    "#     logging.info(param['options']['params']['base_model'])\n",
    "    # Load English parser and text blob (for sentiment analysis)\n",
    "#     model = {}\n",
    "    print(tag + \"Model Initialization: started\")\n",
    "    MODEL_NAME = \"/srv/app/model/data/summarization\"\n",
    "    MODEL_NAME = os.path.join(MODEL_NAME, param['options']['params']['lang'], param['options']['params']['base_model'])\n",
    "    print(tag + \"Model file in \" + MODEL_NAME)\n",
    "#     if param['options']['params']['lang'] == \"jp\":\n",
    "    model = T5FineTuner(MODEL_NAME)\n",
    "    model = model.to(device)\n",
    "    print(tag + \"Model Initialization: successfully finished\")\n",
    "    # GPU memory calculation\n",
    "    t = torch.cuda.get_device_properties(0).total_memory\n",
    "    r = torch.cuda.memory_reserved(0)\n",
    "    a = torch.cuda.memory_allocated(0)\n",
    "    f = r-a  # free inside reserved\n",
    "    load1, load5, load15 = psutil.getloadavg()\n",
    "    cpu_usage = (load15/os.cpu_count()) * 100\n",
    "    stat = shutil.disk_usage(\"/\")\n",
    "    \n",
    "    print(tag + \"#GPU memory --Total memory: {}, --Memory reserved: {}, --Memory allocated: {}. #CPU: {}% occupied. #disk {}\".format(t,r,a,cpu_usage,stat))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- process=fine_tuning_progress model=t5_summarization_en max_epoch=10 -- Training data loaded with shape: (879, 2)\n",
      "-- process=fine_tuning_progress model=t5_summarization_en max_epoch=10 -- Input parameters:  {'algo': 'appNLP_summarization_final', 'max_epochs': '10', 'mode': 'stage', 'lang': 'en', 'base_model': 't5_summarization_en', 'batch_size': '4'}\n",
      "-- process=fine_tuning_progress model=t5_summarization_en max_epoch=10 -- Epoch number: 10\n",
      "-- process=fine_tuning_progress model=t5_summarization_en max_epoch=10 -- Base model: t5_summarization_en\n",
      "-- process=fine_tuning_progress model=t5_summarization_en max_epoch=10 -- Model Initialization: started\n",
      "-- process=fine_tuning_progress model=t5_summarization_en max_epoch=10 -- Model file in /srv/app/model/data/summarization/en/t5_summarization_en\n",
      "-- process=fine_tuning_progress model=t5_summarization_en max_epoch=10 -- Model Initialization: successfully finished\n",
      "-- process=fine_tuning_progress model=t5_summarization_en max_epoch=10 -- #GPU memory --Total memory: 15634661376, --Memory reserved: 1965031424, --Memory allocated: 1783228416. #CPU: 0.25% occupied. #disk usage(total=156052275200, used=155299745792, free=735752192)\n"
     ]
    }
   ],
   "source": [
    "model = init(df,param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3 - fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "name": "mltkc_fit"
   },
   "outputs": [],
   "source": [
    "def fit(model,df,param):  \n",
    "    tag = \"-- process=fine_tuning_progress model={} max_epoch={} -- \".format(param['options']['params']['base_model'], param['options']['params']['max_epochs'])\n",
    "    if \"batch_size\" in param['options']['params']:\n",
    "        print(tag + \"setting batch size to \", param['options']['params']['batch_size'])\n",
    "        batch_size_train = int(param['options']['params']['batch_size'])\n",
    "        batch_size_valid = int(param['options']['params']['batch_size'])\n",
    "    # Data preparation\n",
    "#     if isDialog:\n",
    "#         print(tag + \"Convert dialog tags cu and op into {} and {}\".format(param['options']['params']['dialog_a'], param['options']['params']['dialog_b']))\n",
    "\n",
    "#     def tag_name(text):\n",
    "#         dialog_a = param['options']['params']['dialog_a']\n",
    "#         dialog_b = param['options']['params']['dialog_b']\n",
    "#         text = text.replace(\"cu: \",dialog_a).replace(\"oper: \",dialog_b)\n",
    "#         return text\n",
    "\n",
    "    def preprocess_text(text):\n",
    "        text = re.sub(r'[\\r\\t\\n\\u3000]', '', text)\n",
    "    #     text = neologdn.normalize(text)\n",
    "        text = text.lower()\n",
    "        text = text.strip()\n",
    "        return text\n",
    "\n",
    "    data = df.query('text.notnull()', engine='python').query('summary.notnull()', engine='python')\n",
    "\n",
    "    data = data.assign(\n",
    "        text=lambda x: x.text.map(lambda y: preprocess_text(y)),\n",
    "        summary=lambda x: x.summary.map(lambda y: preprocess_text(y)))\n",
    "    # Data conversion\n",
    "    def convert_batch_data(train_data, valid_data, tokenizer):\n",
    "\n",
    "        def generate_batch(data):\n",
    "\n",
    "            batch_src, batch_tgt = [], []\n",
    "            for src, tgt in data:\n",
    "                batch_src.append(src)\n",
    "                batch_tgt.append(tgt)\n",
    "\n",
    "            batch_src = tokenizer(\n",
    "                batch_src, max_length=max_length_src, truncation=True, padding=\"max_length\", return_tensors=\"pt\"\n",
    "            )\n",
    "            batch_tgt = tokenizer(\n",
    "                batch_tgt, max_length=max_length_target, truncation=True, padding=\"max_length\", return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            return batch_src, batch_tgt\n",
    "\n",
    "        train_iter = DataLoader(train_data, batch_size=batch_size_train, shuffle=True, collate_fn=generate_batch)\n",
    "        valid_iter = DataLoader(valid_data, batch_size=batch_size_valid, shuffle=True, collate_fn=generate_batch)\n",
    "\n",
    "        return train_iter, valid_iter\n",
    "    MODEL_NAME = \"/srv/app/model/data/summarization\"\n",
    "    MODEL_NAME = os.path.join(MODEL_NAME, param['options']['params']['lang'], param['options']['params']['base_model'])\n",
    "    tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME, is_fast=True)\n",
    "    print(tag + \"tokenizer intialized\")\n",
    "    print(tag + \"Data vectorization: started\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        data['text'], data['summary'], test_size=0.15, random_state=42, shuffle=True\n",
    "    )\n",
    "\n",
    "    train_data = [(src, tgt) for src, tgt in zip(X_train, y_train)]\n",
    "    valid_data = [(src, tgt) for src, tgt in zip(X_test, y_test)]\n",
    "\n",
    "    train_iter, valid_iter = convert_batch_data(train_data, valid_data, tokenizer)\n",
    "    print(tag + \"Data vectorization: finished.\")\n",
    "    print(tag + \"#Training data: \" + str(len(train_data)) + \", #Test data: \" + str(len(valid_data)))\n",
    "\n",
    "    # Training function\n",
    "    def train(model, data, optimizer, PAD_IDX, i):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        loop = 1\n",
    "        total = len(data)\n",
    "        losses = 0\n",
    "#         pbar = tqdm(data)\n",
    "#         file=sys.stdout\n",
    "        for src, tgt in data:\n",
    "#             pbar.set_description(tag)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            labels = tgt['input_ids'].to(device)\n",
    "            labels[labels[:, :] == PAD_IDX] = -100\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=src['input_ids'].to(device),\n",
    "                attention_mask=src['attention_mask'].to(device),\n",
    "                decoder_attention_mask=tgt['attention_mask'].to(device),\n",
    "                labels=labels\n",
    "            )\n",
    "            loss = outputs['loss']\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses += loss.item()\n",
    "\n",
    "#             pbar.set_postfix(loss=losses / loop)\n",
    "            print(tag + \"Processed {}% of the {}-th epoch. Finished {} out of {} batches. Loss: {} \".format(round(loop/total*100), i, loop, total, round(losses / loop,2)), flush=True)\n",
    "            loop += 1\n",
    "    #         logger.log()\n",
    "\n",
    "        return losses / len(data)\n",
    "\n",
    "    # Loss function\n",
    "    def evaluate(model, data, PAD_IDX):\n",
    "\n",
    "        model.eval()\n",
    "        losses = 0\n",
    "        with torch.no_grad():\n",
    "            for src, tgt in data:\n",
    "\n",
    "                labels = tgt['input_ids'].to(device)\n",
    "                labels[labels[:, :] == PAD_IDX] = -100\n",
    "\n",
    "                outputs = model(\n",
    "                    input_ids=src['input_ids'].to(device),\n",
    "                    attention_mask=src['attention_mask'].to(device),\n",
    "                    decoder_attention_mask=tgt['attention_mask'].to(device),\n",
    "                    labels=labels\n",
    "                )\n",
    "                loss = outputs['loss']\n",
    "                losses += loss.item()\n",
    "\n",
    "        return losses / len(data)\n",
    "\n",
    "    epochs = int(param['options']['params']['max_epochs'])\n",
    "#     epochs = int(param['options']['params']['epochs'])\n",
    "    MODEL_DIRECTORY = \"/srv/app/model/data/summarization\"\n",
    "    MODEL_DIRECTORY = os.path.join(MODEL_DIRECTORY, param['options']['params']['lang'], param['options']['model_name'])\n",
    "    \n",
    "#     MODEL_DIRECTORY = os.path.join(MODEL_DIRECTORY, param['options']['params']['output'])\n",
    "#     writer = SummaryWriter(log_dir=\"/srv/notebooks/logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    PAD_IDX = tokenizer.pad_token_id\n",
    "    best_loss = float('Inf')\n",
    "    best_model = None\n",
    "    counter = 1\n",
    "\n",
    "    print(tag + 'Model fine-tuning started with {} epochs'.format(epochs))\n",
    "\n",
    "    for loop in range(1, epochs + 1):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        loss_train = train(model=model, data=train_iter, optimizer=optimizer, PAD_IDX=PAD_IDX, i=loop)\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        loss_valid = evaluate(model=model, data=valid_iter, PAD_IDX=PAD_IDX)\n",
    "        \n",
    "#         writer.add_scalar(\"Loss/train\", loss_train, loop)\n",
    "#         writer.add_scalar(\"Loss/valid\", loss_valid, loop)\n",
    "        t = torch.cuda.get_device_properties(0).total_memory\n",
    "        r = torch.cuda.memory_reserved(0)\n",
    "        a = torch.cuda.memory_allocated(0)\n",
    "        f = r-a  # free inside reserved\n",
    "        load1, load5, load15 = psutil.getloadavg()\n",
    "        cpu_usage = (load15/os.cpu_count()) * 100\n",
    "        stat = shutil.disk_usage(\"/\")\n",
    "        print(tag + \"#GPU memory --Total memory: {}, --Memory reserved: {}, --Memory allocated: {}. #CPU: {}% occupied. #disk {}\".format(t,r,a,cpu_usage,stat), flush=True)\n",
    "\n",
    "        print(tag + '[{}/{}] train loss: {:.4f}, valid loss: {:.4f} [{}{:.0f}s] counter: {} {}'.format(\n",
    "            loop, epochs, loss_train, loss_valid,\n",
    "            str(int(math.floor(elapsed_time / 60))) + 'm' if math.floor(elapsed_time / 60) > 0 else '',\n",
    "            elapsed_time % 60,\n",
    "            counter,\n",
    "            '**' if best_loss > loss_valid else ''\n",
    "        ),flush=True)\n",
    "\n",
    "        if best_loss > loss_valid:\n",
    "            best_loss = loss_valid\n",
    "            best_model = copy.deepcopy(model)\n",
    "            counter = 1\n",
    "        else:\n",
    "            if counter > patience:\n",
    "                break\n",
    "\n",
    "            counter += 1\n",
    "        # removing old model file\n",
    "#         os.rmdir(\"myfolder\")\n",
    "        # saving model and tokenizer\n",
    "#         tokenizer.save_pretrained(os.path.join(MODEL_DIRECTORY,'epoch'+str(loop)))\n",
    "#         logging.info(\"tokenizer saved in \" + os.path.join(MODEL_DIRECTORY,'epoch'+str(loop)))\n",
    "#         best_model.model.save_pretrained(os.path.join(MODEL_DIRECTORY,'epoch'+str(loop)))\n",
    "#         logging.info(\"model saved in \" + os.path.join(MODEL_DIRECTORY,'epoch'+str(loop)))\n",
    "        tokenizer.save_pretrained(MODEL_DIRECTORY)\n",
    "        print(tag + \"tokenizer saved in \" + MODEL_DIRECTORY, flush=True)\n",
    "        best_model.model.save_pretrained(MODEL_DIRECTORY)\n",
    "        print(tag + \"model saved in \" + MODEL_DIRECTORY, flush=True)\n",
    "\n",
    "    print(tag + \"Model fine-tuning successfully finished\")\n",
    "#     writer.close()\n",
    "    returns = {}\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- process=fine_tuning_progress model=t5_dialog_jp max_epoch=3 -- setting batch size to  4\n",
      "-- process=fine_tuning_progress model=t5_dialog_jp max_epoch=3 -- tokenizer intialized\n",
      "-- process=fine_tuning_progress model=t5_dialog_jp max_epoch=3 -- Data vectorization: started\n",
      "-- process=fine_tuning_progress model=t5_dialog_jp max_epoch=3 -- Data vectorization: finished.\n",
      "-- process=fine_tuning_progress model=t5_dialog_jp max_epoch=3 -- #Training data: 85, #Test data: 15\n",
      "-- process=fine_tuning_progress model=t5_dialog_jp max_epoch=3 -- Model fine-tuning started with 3 epochs\n",
      "-- process=fine_tuning_progress model=t5_dialog_jp max_epoch=3 -- Processed 5% of the 1-th epoch. Finished 1 out of 22 batches. Loss: 1.14 \n",
      "-- process=fine_tuning_progress model=t5_dialog_jp max_epoch=3 -- Processed 9% of the 1-th epoch. Finished 2 out of 22 batches. Loss: 1.43 \n",
      "-- process=fine_tuning_progress model=t5_dialog_jp max_epoch=3 -- Processed 14% of the 1-th epoch. Finished 3 out of 22 batches. Loss: 1.54 \n",
      "-- process=fine_tuning_progress model=t5_dialog_jp max_epoch=3 -- Processed 18% of the 1-th epoch. Finished 4 out of 22 batches. Loss: 1.47 \n",
      "-- process=fine_tuning_progress model=t5_dialog_jp max_epoch=3 -- Processed 23% of the 1-th epoch. Finished 5 out of 22 batches. Loss: 1.49 \n",
      "-- process=fine_tuning_progress model=t5_dialog_jp max_epoch=3 -- Processed 27% of the 1-th epoch. Finished 6 out of 22 batches. Loss: 1.45 \n",
      "-- process=fine_tuning_progress model=t5_dialog_jp max_epoch=3 -- Processed 32% of the 1-th epoch. Finished 7 out of 22 batches. Loss: 1.44 \n",
      "-- process=fine_tuning_progress model=t5_dialog_jp max_epoch=3 -- Processed 36% of the 1-th epoch. Finished 8 out of 22 batches. Loss: 1.48 \n",
      "-- process=fine_tuning_progress model=t5_dialog_jp max_epoch=3 -- Processed 41% of the 1-th epoch. Finished 9 out of 22 batches. Loss: 1.57 \n",
      "-- process=fine_tuning_progress model=t5_dialog_jp max_epoch=3 -- Processed 45% of the 1-th epoch. Finished 10 out of 22 batches. Loss: 1.56 \n",
      "-- process=fine_tuning_progress model=t5_dialog_jp max_epoch=3 -- Processed 50% of the 1-th epoch. Finished 11 out of 22 batches. Loss: 1.55 \n",
      "-- process=fine_tuning_progress model=t5_dialog_jp max_epoch=3 -- Processed 55% of the 1-th epoch. Finished 12 out of 22 batches. Loss: 1.58 \n",
      "-- process=fine_tuning_progress model=t5_dialog_jp max_epoch=3 -- Processed 59% of the 1-th epoch. Finished 13 out of 22 batches. Loss: 1.58 \n",
      "-- process=fine_tuning_progress model=t5_dialog_jp max_epoch=3 -- Processed 64% of the 1-th epoch. Finished 14 out of 22 batches. Loss: 1.57 \n",
      "-- process=fine_tuning_progress model=t5_dialog_jp max_epoch=3 -- Processed 68% of the 1-th epoch. Finished 15 out of 22 batches. Loss: 1.59 \n",
      "-- process=fine_tuning_progress model=t5_dialog_jp max_epoch=3 -- Processed 73% of the 1-th epoch. Finished 16 out of 22 batches. Loss: 1.59 \n",
      "-- process=fine_tuning_progress model=t5_dialog_jp max_epoch=3 -- Processed 77% of the 1-th epoch. Finished 17 out of 22 batches. Loss: 1.59 \n",
      "-- process=fine_tuning_progress model=t5_dialog_jp max_epoch=3 -- Processed 82% of the 1-th epoch. Finished 18 out of 22 batches. Loss: 1.59 \n",
      "-- process=fine_tuning_progress model=t5_dialog_jp max_epoch=3 -- Processed 86% of the 1-th epoch. Finished 19 out of 22 batches. Loss: 1.58 \n",
      "-- process=fine_tuning_progress model=t5_dialog_jp max_epoch=3 -- Processed 91% of the 1-th epoch. Finished 20 out of 22 batches. Loss: 1.58 \n",
      "-- process=fine_tuning_progress model=t5_dialog_jp max_epoch=3 -- Processed 95% of the 1-th epoch. Finished 21 out of 22 batches. Loss: 1.59 \n",
      "-- process=fine_tuning_progress model=t5_dialog_jp max_epoch=3 -- Processed 100% of the 1-th epoch. Finished 22 out of 22 batches. Loss: 1.66 \n",
      "-- process=fine_tuning_progress model=t5_dialog_jp max_epoch=3 -- #GPU memory --Total memory: 15634661376, --Memory reserved: 8476688384, --Memory allocated: 3613427712. #CPU: 0.5% occupied. #disk usage(total=156052275200, used=153944645632, free=2090852352)\n",
      "-- process=fine_tuning_progress model=t5_dialog_jp max_epoch=3 -- [1/3] train loss: 1.6564, valid loss: 1.6993 [14s] counter: 1 **\n",
      "-- process=fine_tuning_progress model=t5_dialog_jp max_epoch=3 -- tokenizer saved in /srv/app/model/data/summarization/jp/t5_dialog_jp_finetuned_final\n",
      "-- process=fine_tuning_progress model=t5_dialog_jp max_epoch=3 -- model saved in /srv/app/model/data/summarization/jp/t5_dialog_jp_finetuned_final\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 100.00 MiB (GPU 0; 14.56 GiB total capacity; 7.94 GiB already allocated; 43.50 MiB free; 8.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-e0fe780e5f4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-c04df096c1ac>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, df, param)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mloss_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPAD_IDX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPAD_IDX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-c04df096c1ac>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data, optimizer, PAD_IDX, i)\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mlosses\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 0; 14.56 GiB total capacity; 7.94 GiB already allocated; 43.50 MiB free; 8.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "fit(model,df,param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4 - apply the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "name": "mltkc_apply"
   },
   "outputs": [],
   "source": [
    "def apply(model,df,param):\n",
    "    print(\"DEBUG: enter apply\")\n",
    "    print(param)\n",
    "    tag = \"-- process=fine_tuning_progress model={} max_epoch={} -- \".format(param['options']['params']['base_model'], param['options']['params']['max_epochs'])\n",
    "    MODEL_DIRECTORY = \"/srv/app/model/data/summarization\"\n",
    "    MODEL_DIRECTORY = os.path.join(MODEL_DIRECTORY, param['options']['params']['lang'], param['options']['model_name'])\n",
    "    model = {}\n",
    "    print(MODEL_DIRECTORY)\n",
    "    model[\"tokenizer\"] = T5Tokenizer.from_pretrained(MODEL_DIRECTORY)\n",
    "    model[\"summarizer\"] = T5ForConditionalGeneration.from_pretrained(MODEL_DIRECTORY)\n",
    "    print(\"DEBUG: model inited\")\n",
    "    X = df[param['feature_variables'][0]].values.tolist()\n",
    "#     print(\"input X loaded\")\n",
    "#     Y = df[\"summary\"].values.tolist()\n",
    "#     print(\"input y loaded\")\n",
    "#     Z = [x[0] for x in X]\n",
    "    temp_data=list()\n",
    "#     temp_rouge=list()\n",
    "#     rouge = ROUGEScore()\n",
    "    \n",
    "#     tokenizer = AutoTokenizer.from_pretrained(\"/srv/app/model/data/samsum_tok\")\n",
    "#     summarizer = AutoModelForSeq2SeqLM.from_pretrained(\"/srv/app/model/data/samsum\")\n",
    "    print(tag + \"apply function read inputs\")\n",
    "    for i in range(len(X)):\n",
    "        batch = model[\"tokenizer\"](str(X[i]), max_length=400, truncation=True, return_tensors=\"pt\")\n",
    "        outputs = model[\"summarizer\"].generate(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], max_length=400,repetition_penalty=8.0,num_beams=15)\n",
    "        summary = [model[\"tokenizer\"].decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=False) for ids in outputs]\n",
    "#         r = rouge(summary, Y[i])\n",
    "        temp_data += summary\n",
    "#         temp_rouge.append(1)\n",
    "#         temp_rouge.append(round(r[param['options']['params']['metrics']].item(),2))\n",
    "#     return temp_data\n",
    "#     column_names=[param['target_variables'], param['options']['params']['metrics']]\n",
    "#     cols={\"summary\": temp_data, param['options']['params']['metrics']: temp_rouge}\n",
    "    cols={\"summary\": temp_data}\n",
    "    returns=pd.DataFrame(data=cols)\n",
    "#     returns=pd.DataFrame(temp_data, temp_rouge, columns=column_names)\n",
    "    print(tag + \"apply function successfully finished\")\n",
    "        \n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug: start apply\n",
      "debug: read input\n",
      "numpy version: 1.22.1\n",
      "debug: start tokenizing\n",
      "debug: finish tok and start summarizing\n",
      "debug: finish summarizing and start decoding\n",
      "debug: finish decoding\n",
      "debug: finished\n",
      "無線のプレミアムバージョンに加入しているユーザー番号は100145。通常加入よりも高速であるはずなのに、とても遅いという。お客様が契約したパッケージの説明とまったく同じだそう\n"
     ]
    }
   ],
   "source": [
    "returns = apply(model,df,param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 5 - save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "name": "mltkc_save"
   },
   "outputs": [],
   "source": [
    "# save model to name in expected convention \"<algo_name>_<model_name>.h5\"\n",
    "def save(model, name):\n",
    "    return {}\n",
    "#     model_dir_path = Path(path)\n",
    "#     tokenizer.save_pretrained(model_dir_path)\n",
    "#     print(\"tokenizer saved.\")\n",
    "#     best_model.model.save_pretrained(model_dir_path)\n",
    "#     print(\"model saved. Successfully finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 6 - load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "name": "mltkc_load"
   },
   "outputs": [],
   "source": [
    "# load model from name in expected convention \"<algo_name>_<model_name>.h5\"\n",
    "def load(path):\n",
    "    print(\"DEBUG: load\")\n",
    "    model = {}\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 7 - provide a summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "name": "mltkc_summary"
   },
   "outputs": [],
   "source": [
    "# return model summary\n",
    "def summary(model=None):\n",
    "    returns = {}\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Stages\n",
    "All subsequent cells are not tagged and can be used for further freeform code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
