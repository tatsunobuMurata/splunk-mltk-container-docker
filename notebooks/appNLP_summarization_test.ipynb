{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization fine-tuning script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script contains the source code of how to fine-tune a t5 model on **End-to-End automatic summarization task** in both English and Japanese."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 0 - import libraries\n",
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "name": "mltkc_import"
   },
   "outputs": [],
   "source": [
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",
    "# import sys\n",
    "# sys.path.insert(1, '/opt/conda/lib/python3.8/site-packages')\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "# import neologdn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from torchmetrics.text.rouge import ROUGEScore\n",
    "# tensorboard related\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tensorboard\n",
    "import datetime\n",
    "import logging\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "import psutil\n",
    "import shutil\n",
    "# Fine-tune parameters initialization\n",
    "MODEL_NAME = \"/srv/app/model/data\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "max_length_src = 400\n",
    "max_length_target = 200\n",
    "\n",
    "batch_size_train = 4\n",
    "batch_size_valid = 4\n",
    "\n",
    "epochs = 100\n",
    "patience = 20\n",
    "\n",
    "MODEL_DIRECTORY = \"/\"\n",
    "\n",
    "class T5FineTuner(nn.Module):\n",
    "    \n",
    "    def __init__(self, MODEL_NAME):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids, attention_mask=None, decoder_input_ids=None,\n",
    "        decoder_attention_mask=None, labels=None\n",
    "    ):\n",
    "        return self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "    \n",
    "class BartFineTuner(nn.Module):\n",
    "    \n",
    "    def __init__(self, MODEL_NAME):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids, attention_mask=None, decoder_input_ids=None,\n",
    "        decoder_attention_mask=None, labels=None\n",
    "    ):\n",
    "        return self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            labels=labels\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1 - get a data sample from Splunk\n",
    "In Splunk run a search to pipe a prepared dataset into this environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| inputlookup summarization_en\n",
    "| fields text summary\n",
    "| head 5\n",
    "| fit MLTKContainer algo=appNLP_summarization_test max_epochs=1 lang=en base_model=t5_summarization_en metrics=rouge1_fmeasure type=dialog dialog_a=cust: dialog_b=op: summary from text into app:t5_summarization_en_finetuned_test as extracted_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "name": "mltkc_stage"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-06 08:20:35,664 process=fine_tuning_progress model=t5_dialog_jp epoch=2 Training data loaded with shape: (10, 2)\n",
      "2022-10-06 08:20:35,665 process=fine_tuning_progress model=t5_dialog_jp epoch=2 Input parameters: \n",
      "2022-10-06 08:20:35,665 process=fine_tuning_progress model=t5_dialog_jp epoch=2 {'algo': 'appNLP_summarization', 'mode': 'stage', 'max_epochs': '2', 'lang': 'jp', 'base_model': 't5_dialog_jp', 'type': 'dialog', 'dialog_a': 'cust:', 'dialog_b': 'op:'}\n",
      "2022-10-06 08:20:35,666 process=fine_tuning_progress model=t5_dialog_jp epoch=2 Epoch number: 2\n",
      "2022-10-06 08:20:35,667 process=fine_tuning_progress model=t5_dialog_jp epoch=2 Base model: t5_dialog_jp\n"
     ]
    }
   ],
   "source": [
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",
    "def stage(name):\n",
    "    print(\"DEBUG stage call\")\n",
    "    print(\"DEBUG\" + name)\n",
    "    with open(\"/srv/notebooks/data/\"+name+\".csv\", 'r') as f:\n",
    "        df = pd.read_csv(f)\n",
    "    with open(\"/srv/notebooks/data/\"+name+\".json\", 'r') as f:\n",
    "        param = json.load(f) \n",
    "#         param = {}\n",
    "    return df, param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 - create and initialize a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "name": "mltkc_init"
   },
   "outputs": [],
   "source": [
    "def init(df,param):\n",
    "    tag = \"-- process=fine_tuning_progress model={} max_epoch={} -- \".format(param['options']['params']['base_model'], param['options']['params']['max_epochs'])\n",
    "#     df, param = df, param\n",
    "#     if df is not None:\n",
    "#         print(\"received df\")\n",
    "#     if param is not None:\n",
    "#         print(\"received param\")\n",
    "#         print(param)\n",
    "#     print(\"DEBUG init call\")\n",
    "\n",
    "    print(tag + \"Training data loaded with shape: \" + str(df.shape))\n",
    "    print(tag + \"Input parameters: \", param['options']['params'])\n",
    "    print(tag + \"Epoch number: \" + param['options']['params']['max_epochs'])\n",
    "    print(tag + \"Base model: \" + param['options']['params']['base_model'])\n",
    "    \n",
    "#     logging.info(param['options']['params']['base_model'])\n",
    "    # Load English parser and text blob (for sentiment analysis)\n",
    "#     model = {}\n",
    "    print(tag + \"Model Initialization: started\")\n",
    "    MODEL_NAME = \"/srv/app/model/data/summarization\"\n",
    "    MODEL_NAME = os.path.join(MODEL_NAME, param['options']['params']['lang'], param['options']['params']['base_model'])\n",
    "    print(tag + \"Model file in \" + MODEL_NAME)\n",
    "#     if param['options']['params']['lang'] == \"jp\":\n",
    "    model = T5FineTuner(MODEL_NAME)\n",
    "    model = model.to(device)\n",
    "    print(tag + \"Model Initialization: successfully finished\")\n",
    "    # GPU memory calculation\n",
    "    t = torch.cuda.get_device_properties(0).total_memory\n",
    "    r = torch.cuda.memory_reserved(0)\n",
    "    a = torch.cuda.memory_allocated(0)\n",
    "    f = r-a  # free inside reserved\n",
    "    load1, load5, load15 = psutil.getloadavg()\n",
    "    cpu_usage = (load15/os.cpu_count()) * 100\n",
    "    stat = shutil.disk_usage(\"/\")\n",
    "    \n",
    "    print(tag + \"#GPU memory --Total memory: {}, --Memory reserved: {}, --Memory allocated: {}. #CPU: {}% occupied. #disk {}\".format(t,r,a,cpu_usage,stat))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-06 08:20:46,966 process=fine_tuning_progress model=t5_dialog_jp epoch=2 Model Initialization: started\n",
      "2022-10-06 08:20:46,968 process=fine_tuning_progress model=t5_dialog_jp epoch=2 Model file in /srv/app/model/data/summarization/jp/t5_dialog_jp\n",
      "2022-10-06 08:20:54,602 process=fine_tuning_progress model=t5_dialog_jp epoch=2 Model Initialization: successfully finished\n"
     ]
    }
   ],
   "source": [
    "model = init(df,param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3 - fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "name": "mltkc_fit"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-06 08:20:59,163 process=fine_tuning_progress model=t5_dialog_jp epoch=2 Convert dialog tags cu and op into cust: and op:\n",
      "2022-10-06 08:20:59,244 process=fine_tuning_progress model=t5_dialog_jp epoch=2 Data vectorization: started\n",
      "2022-10-06 08:20:59,247 process=fine_tuning_progress model=t5_dialog_jp epoch=2 Data vectorization: finished.\n",
      "2022-10-06 08:20:59,248 process=fine_tuning_progress model=t5_dialog_jp epoch=2 #Training data: 8, #Test data: 2\n"
     ]
    }
   ],
   "source": [
    "def fit(model,df,param):  \n",
    "    tag = \"-- process=fine_tuning_progress model={} max_epoch={} -- \".format(param['options']['params']['base_model'], param['options']['params']['max_epochs'])\n",
    "    # Data preparation\n",
    "    isDialog = (param['options']['params']['type'] == 'dialog')\n",
    "    if isDialog:\n",
    "        print(tag + \"Convert dialog tags cu and op into {} and {}\".format(param['options']['params']['dialog_a'], param['options']['params']['dialog_b']))\n",
    "\n",
    "    def tag_name(text):\n",
    "        dialog_a = param['options']['params']['dialog_a']\n",
    "        dialog_b = param['options']['params']['dialog_b']\n",
    "        text = text.replace(\"cu: \",dialog_a).replace(\"oper: \",dialog_b)\n",
    "        return text\n",
    "\n",
    "    def preprocess_text(text):\n",
    "        text = re.sub(r'[\\r\\t\\n\\u3000]', '', text)\n",
    "    #     text = neologdn.normalize(text)\n",
    "        text = text.lower()\n",
    "        text = text.strip()\n",
    "        return text\n",
    "\n",
    "    data = df.query('text.notnull()', engine='python').query('summary.notnull()', engine='python')\n",
    "\n",
    "    data = data.assign(\n",
    "        text=lambda x: x.text.map(lambda y: tag_name(preprocess_text(y)) if isDialog else preprocess_text(y)),\n",
    "        summary=lambda x: x.summary.map(lambda y: preprocess_text(y)))\n",
    "    # Data conversion\n",
    "    def convert_batch_data(train_data, valid_data, tokenizer):\n",
    "\n",
    "        def generate_batch(data):\n",
    "\n",
    "            batch_src, batch_tgt = [], []\n",
    "            for src, tgt in data:\n",
    "                batch_src.append(src)\n",
    "                batch_tgt.append(tgt)\n",
    "\n",
    "            batch_src = tokenizer(\n",
    "                batch_src, max_length=max_length_src, truncation=True, padding=\"max_length\", return_tensors=\"pt\"\n",
    "            )\n",
    "            batch_tgt = tokenizer(\n",
    "                batch_tgt, max_length=max_length_target, truncation=True, padding=\"max_length\", return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            return batch_src, batch_tgt\n",
    "\n",
    "        train_iter = DataLoader(train_data, batch_size=batch_size_train, shuffle=True, collate_fn=generate_batch)\n",
    "        valid_iter = DataLoader(valid_data, batch_size=batch_size_valid, shuffle=True, collate_fn=generate_batch)\n",
    "\n",
    "        return train_iter, valid_iter\n",
    "    MODEL_NAME = \"/srv/app/model/data/summarization\"\n",
    "    MODEL_NAME = os.path.join(MODEL_NAME, param['options']['params']['lang'], param['options']['params']['base_model'])\n",
    "    tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME, is_fast=True)\n",
    "    print(tag + \"tokenizer intialized\")\n",
    "    print(tag + \"Data vectorization: started\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        data['text'], data['summary'], test_size=0.15, random_state=42, shuffle=True\n",
    "    )\n",
    "\n",
    "    train_data = [(src, tgt) for src, tgt in zip(X_train, y_train)]\n",
    "    valid_data = [(src, tgt) for src, tgt in zip(X_test, y_test)]\n",
    "\n",
    "    train_iter, valid_iter = convert_batch_data(train_data, valid_data, tokenizer)\n",
    "    print(tag + \"Data vectorization: finished.\")\n",
    "    print(tag + \"#Training data: \" + str(len(train_data)) + \", #Test data: \" + str(len(valid_data)))\n",
    "\n",
    "    # Training function\n",
    "    def train(model, data, optimizer, PAD_IDX):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        loop = 1\n",
    "        losses = 0\n",
    "        pbar = tqdm(data, file=sys.stdout)\n",
    "        for src, tgt in pbar:\n",
    "            pbar.set_description(tag)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            labels = tgt['input_ids'].to(device)\n",
    "            labels[labels[:, :] == PAD_IDX] = -100\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=src['input_ids'].to(device),\n",
    "                attention_mask=src['attention_mask'].to(device),\n",
    "                decoder_attention_mask=tgt['attention_mask'].to(device),\n",
    "                labels=labels\n",
    "            )\n",
    "            loss = outputs['loss']\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses += loss.item()\n",
    "\n",
    "            pbar.set_postfix(loss=losses / loop)\n",
    "            loop += 1\n",
    "    #         logger.log()\n",
    "\n",
    "        return losses / len(data)\n",
    "\n",
    "    # Loss function\n",
    "    def evaluate(model, data, PAD_IDX):\n",
    "\n",
    "        model.eval()\n",
    "        losses = 0\n",
    "        with torch.no_grad():\n",
    "            for src, tgt in data:\n",
    "\n",
    "                labels = tgt['input_ids'].to(device)\n",
    "                labels[labels[:, :] == PAD_IDX] = -100\n",
    "\n",
    "                outputs = model(\n",
    "                    input_ids=src['input_ids'].to(device),\n",
    "                    attention_mask=src['attention_mask'].to(device),\n",
    "                    decoder_attention_mask=tgt['attention_mask'].to(device),\n",
    "                    labels=labels\n",
    "                )\n",
    "                loss = outputs['loss']\n",
    "                losses += loss.item()\n",
    "\n",
    "        return losses / len(data)\n",
    "\n",
    "    epochs = int(param['options']['params']['max_epochs'])\n",
    "#     epochs = int(param['options']['params']['epochs'])\n",
    "    MODEL_DIRECTORY = \"/srv/app/model/data/summarization\"\n",
    "    MODEL_DIRECTORY = os.path.join(MODEL_DIRECTORY, param['options']['params']['lang'], param['options']['model_name'])\n",
    "    \n",
    "#     MODEL_DIRECTORY = os.path.join(MODEL_DIRECTORY, param['options']['params']['output'])\n",
    "#     writer = SummaryWriter(log_dir=\"/srv/notebooks/logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    PAD_IDX = tokenizer.pad_token_id\n",
    "    best_loss = float('Inf')\n",
    "    best_model = None\n",
    "    counter = 1\n",
    "\n",
    "    print(tag + 'Model fine-tuning started with {} epochs'.format(epochs))\n",
    "\n",
    "    for loop in range(1, epochs + 1):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        loss_train = train(model=model, data=train_iter, optimizer=optimizer, PAD_IDX=PAD_IDX)\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        loss_valid = evaluate(model=model, data=valid_iter, PAD_IDX=PAD_IDX)\n",
    "        \n",
    "#         writer.add_scalar(\"Loss/train\", loss_train, loop)\n",
    "#         writer.add_scalar(\"Loss/valid\", loss_valid, loop)\n",
    "        t = torch.cuda.get_device_properties(0).total_memory\n",
    "        r = torch.cuda.memory_reserved(0)\n",
    "        a = torch.cuda.memory_allocated(0)\n",
    "        f = r-a  # free inside reserved\n",
    "        load1, load5, load15 = psutil.getloadavg()\n",
    "        cpu_usage = (load15/os.cpu_count()) * 100\n",
    "        stat = shutil.disk_usage(\"/\")\n",
    "        print(tag + \"#GPU memory --Total memory: {}, --Memory reserved: {}, --Memory allocated: {}. #CPU: {}% occupied. #disk {}\".format(t,r,a,cpu_usage,stat))\n",
    "\n",
    "        print(tag + '[{}/{}] train loss: {:.4f}, valid loss: {:.4f} [{}{:.0f}s] counter: {} {}'.format(\n",
    "            loop, epochs, loss_train, loss_valid,\n",
    "            str(int(math.floor(elapsed_time / 60))) + 'm' if math.floor(elapsed_time / 60) > 0 else '',\n",
    "            elapsed_time % 60,\n",
    "            counter,\n",
    "            '**' if best_loss > loss_valid else ''\n",
    "        ))\n",
    "\n",
    "        if best_loss > loss_valid:\n",
    "            best_loss = loss_valid\n",
    "            best_model = copy.deepcopy(model)\n",
    "            counter = 1\n",
    "        else:\n",
    "            if counter > patience:\n",
    "                break\n",
    "\n",
    "            counter += 1\n",
    "        # removing old model file\n",
    "#         os.rmdir(\"myfolder\")\n",
    "        # saving model and tokenizer\n",
    "#         tokenizer.save_pretrained(os.path.join(MODEL_DIRECTORY,'epoch'+str(loop)))\n",
    "#         logging.info(\"tokenizer saved in \" + os.path.join(MODEL_DIRECTORY,'epoch'+str(loop)))\n",
    "#         best_model.model.save_pretrained(os.path.join(MODEL_DIRECTORY,'epoch'+str(loop)))\n",
    "#         logging.info(\"model saved in \" + os.path.join(MODEL_DIRECTORY,'epoch'+str(loop)))\n",
    "        tokenizer.save_pretrained(MODEL_DIRECTORY)\n",
    "        print(tag + \"tokenizer saved in \" + MODEL_DIRECTORY)\n",
    "        best_model.model.save_pretrained(MODEL_DIRECTORY)\n",
    "        print(tag + \"model saved in \" + MODEL_DIRECTORY)\n",
    "\n",
    "    print(tag + \"Model fine-tuning successfully finished\")\n",
    "#     writer.close()\n",
    "    returns = {}\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-06 08:21:02,449 process=fine_tuning_progress model=t5_dialog_jp epoch=2 Model fine-tuning: started with 2 epochs\n",
      "2022-10-06 08:21:02,458 process=fine_tuning_progress model=t5_dialog_jp epoch=2 0%|          | 0/2 [00:00<?, ?it/s]\n",
      "2022-10-06 08:21:03,667 process=fine_tuning_progress model=t5_dialog_jp epoch=2 0%|          | 0/2 [00:01<?, ?it/s, loss=1.48]\n",
      "2022-10-06 08:21:03,669 process=fine_tuning_progress model=t5_dialog_jp epoch=2 50%|#####     | 1/2 [00:01<00:01,  1.21s/it, loss=1.48]\n",
      "2022-10-06 08:21:04,269 process=fine_tuning_progress model=t5_dialog_jp epoch=2 50%|#####     | 1/2 [00:01<00:01,  1.21s/it, loss=1.22]\n",
      "2022-10-06 08:21:04,271 process=fine_tuning_progress model=t5_dialog_jp epoch=2 100%|##########| 2/2 [00:01<00:00,  1.03s/it, loss=1.22]\n",
      "2022-10-06 08:21:04,275 process=fine_tuning_progress model=t5_dialog_jp epoch=2 100%|##########| 2/2 [00:01<00:00,  1.10it/s, loss=1.22]\n",
      "2022-10-06 08:21:04,392 process=fine_tuning_progress model=t5_dialog_jp epoch=2 [1/2] train loss: 1.2158, valid loss: 1.5378 [2s] counter: 1 **\n",
      "2022-10-06 08:21:04,489 process=fine_tuning_progress model=t5_dialog_jp epoch=2 tokenizer saved in /srv/app/model/data/summarization/jp/t5_dialog_jp_finetuned\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e0fe780e5f4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-3c564143415c>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, df, param)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_DIRECTORY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tokenizer saved in \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mMODEL_DIRECTORY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_DIRECTORY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model saved in \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mMODEL_DIRECTORY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36msave_pretrained\u001b[0;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, **kwargs)\u001b[0m\n\u001b[1;32m   1561\u001b[0m         \u001b[0;31m# Save the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1562\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mshard_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshard\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1563\u001b[0;31m             \u001b[0msave_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshard_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m         \u001b[0m_legacy_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fit(model,df,param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4 - apply the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "name": "mltkc_apply"
   },
   "outputs": [],
   "source": [
    "def apply(model,df,param):\n",
    "    tag = \"-- process=fine_tuning_progress model={} max_epoch={} -- \".format(param['options']['params']['base_model'], param['options']['params']['max_epochs'])\n",
    "    MODEL_DIRECTORY = \"/srv/app/model/data/summarization\"\n",
    "    MODEL_DIRECTORY = os.path.join(MODEL_DIRECTORY, param['options']['params']['lang'], param['options']['model_name'])\n",
    "    model = {}\n",
    "    model[\"tokenizer\"] = T5Tokenizer.from_pretrained(MODEL_DIRECTORY)\n",
    "    model[\"summarizer\"] = T5ForConditionalGeneration.from_pretrained(MODEL_DIRECTORY)\n",
    "    \n",
    "    X = df[param['feature_variables']].values.tolist()\n",
    "    Y = df[\"summary\"].values.tolist()\n",
    "#     Z = [x[0] for x in X]\n",
    "    temp_data=list()\n",
    "    temp_rouge=list()\n",
    "    rouge = ROUGEScore()\n",
    "    \n",
    "#     tokenizer = AutoTokenizer.from_pretrained(\"/srv/app/model/data/samsum_tok\")\n",
    "#     summarizer = AutoModelForSeq2SeqLM.from_pretrained(\"/srv/app/model/data/samsum\")\n",
    "    print(tag + \"apply function read inputs\")\n",
    "    for i in range(len(X)):\n",
    "        batch = model[\"tokenizer\"](str(X[i]), max_length=400, truncation=True, return_tensors=\"pt\")\n",
    "        outputs = model[\"summarizer\"].generate(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], max_length=400,repetition_penalty=8.0,num_beams=15)\n",
    "        summary = [model[\"tokenizer\"].decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=False) for ids in outputs]\n",
    "        r = rouge(summary, Y[i])\n",
    "        temp_data += summary\n",
    "        temp_rouge.append(round(r[param['options']['params']['metrics']].item(),2))\n",
    "#     return temp_data\n",
    "#     column_names=[param['target_variables'], param['options']['params']['metrics']]\n",
    "    cols={param['target_variables'][0]: temp_data, param['options']['params']['metrics']: temp_rouge}\n",
    "    returns=pd.DataFrame(data=cols)\n",
    "#     returns=pd.DataFrame(temp_data, temp_rouge, columns=column_names)\n",
    "    print(tag + \"apply function successfully finished\")\n",
    "        \n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug: start apply\n",
      "debug: read input\n",
      "numpy version: 1.22.1\n",
      "debug: start tokenizing\n",
      "debug: finish tok and start summarizing\n",
      "debug: finish summarizing and start decoding\n",
      "debug: finish decoding\n",
      "debug: finished\n",
      "無線のプレミアムバージョンに加入しているユーザー番号は100145。通常加入よりも高速であるはずなのに、とても遅いという。お客様が契約したパッケージの説明とまったく同じだそう\n"
     ]
    }
   ],
   "source": [
    "returns = apply(model,df,param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 5 - save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "name": "mltkc_save"
   },
   "outputs": [],
   "source": [
    "# save model to name in expected convention \"<algo_name>_<model_name>.h5\"\n",
    "def save(model, name):\n",
    "    return {}\n",
    "#     model_dir_path = Path(path)\n",
    "#     tokenizer.save_pretrained(model_dir_path)\n",
    "#     print(\"tokenizer saved.\")\n",
    "#     best_model.model.save_pretrained(model_dir_path)\n",
    "#     print(\"model saved. Successfully finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(MODEL_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 6 - load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "name": "mltkc_load"
   },
   "outputs": [],
   "source": [
    "# load model from name in expected convention \"<algo_name>_<model_name>.h5\"\n",
    "def load(path):\n",
    "    model = {}\n",
    "    model[\"tokenizer\"] = T5Tokenizer.from_pretrained(path)\n",
    "    model[\"summarizer\"] = T5ForConditionalGeneration.from_pretrained(path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load(MODEL_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 7 - provide a summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "name": "mltkc_summary"
   },
   "outputs": [],
   "source": [
    "# return model summary\n",
    "def summary(model=None):\n",
    "    returns = {}\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Stages\n",
    "All subsequent cells are not tagged and can be used for further freeform code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
