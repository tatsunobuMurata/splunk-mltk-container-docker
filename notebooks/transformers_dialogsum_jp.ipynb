{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dialogue Summarization with transformer-based pre-trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains an example workflow how to work on custom containerized code that seamlessly interfaces with the Splunk Machine Learning Toolkit (MLTK) Container for TensorFlow. This script contains an example of how to conduct **End-to-End automatic summarization task over English dialogue transcripts** using the transformers library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 0 - import libraries\n",
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "name": "mltkc_import"
   },
   "outputs": [],
   "source": [
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",
    "# import sys\n",
    "# sys.path.insert(1, '/opt/conda/lib/python3.8/site-packages')\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "import os\n",
    "# path = os.path.abspath(np.__file__)\n",
    "# print(\"numpy path in runtime: \" + path)\n",
    "# print(\"numpy version in runtime: \" + np.__version__)\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"/opt/conda/lib/python3.8/site-packages/samsum_tok\", local_files_only=True)\n",
    "# summarizer = AutoModelForSeq2SeqLM.from_pretrained(\"/opt/conda/lib/python3.8/site-packages/samsum\", local_files_only=True)\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"philschmid/bart-large-cnn-samsum\")\n",
    "# # tokenizer.save_pretrained(\"/opt/conda/lib/python3.8/site-packages/samsum_tok\")\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"philschmid/bart-large-cnn-samsum\")\n",
    "# model.save_pretrained(\"/opt/conda/lib/python3.8/site-packages/samsum\")\n",
    "\n",
    "MODEL_DIRECTORY = \"/srv/app/model/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.22.1\n",
      "pandas version: 1.4.3\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing purposes\n",
    "print(\"numpy version: \" + np.__version__)\n",
    "print(\"pandas version: \" + pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "# summarizer = pipeline(\"summarization\", model=\"lidiya/bart-base-samsum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1 - get a data sample from Splunk\n",
    "In Splunk run a search to pipe a prepared dataset into this environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| makeresults\n",
    "| eval text = \"お客様：こんにちは、；従業者：こんにちは、；お客様、ご機嫌いかがでしょうか？；お客様：こんにちは、元気です。私はwifiサービスの性能に大きな問題を抱えています。先月から御社のwifiサービスを使い始めたのですが、購入したサービスのようにうまく機能していないのです。；従業者：今、ユーザー番号を確認します。お客様のユーザー番号は100145で、先月に作成されたものですが、よろしいですか？；お客様：はい ；従業者：お客様さん、何が問題なのか説明してもらえますか？；お客様：はい。私はあなたの無線LANのプレミアムバージョンに加入しており、それは通常の加入よりも高速であるはずです。しかし、とても遅いのです。；従業者：しかし、私たちのセンターから提供する無線LANサービスは、あなたが契約したパッケージの説明とまったく同じです。；お客様：私は自分の目で見た無線LANの速度を信じています。；従業者：申し訳ございませんが、お客様、私はあなたが嘘をついていると非難するつもりは全くありません、私はすぐにこの問題を解決します、あなたは私に詳細に説明することができますか？無線LANがあなたの期待に比べてどのように遅いですか？；お客様様：無線LANは時々オフラインで、物をダウンロードするのに時間がかかります。；従業者：ローカルネットワークに何らかの問題があるかもしれませんね。ルーターをチェックしていただけますか？；お客様: はい。 ；従業者： ごゆっくり、お客様。；お客様：ルーターの電源は入っていますが、ランプが点滅しています ；従業者：ランプが点滅するのは正常ではありません。ランプは接続の安定性を示すものですから、ランプが点滅するのは正常なことではありません。どのランプが点滅しているのか、またその点滅の速さを知る必要があるかもしれませんが、ご確認いただけますか？；お客様：はい ；従業者：アクティブという名前の光はありますか？；お客様：はい。それは点滅しているものです。；従業者：それはあなたのネットワークの問題であるべきですあなたはすべてのケーブルがうまく接続されていることを確認することができますか？；お客様：はい ；従業者：無線LANを再接続しようとしたことがありますか？；お客様：はい、私はやったし、私は問題があなたの側にあると思う　；従業者：今私はあなたのローカルネットワークのデータ入力を確認します\"\n",
    "| fit MLTKContainer algo=transformers_dialogsum_jp mode=stage epochs=100 text into app:transformers_dialogjp_model as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "name": "mltkc_stage"
   },
   "outputs": [],
   "source": [
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",
    "def stage(name):\n",
    "    with open(\"data/\"+name+\".csv\", 'r') as f:\n",
    "        df = pd.read_csv(f)\n",
    "    with open(\"data/\"+name+\".json\", 'r') as f:\n",
    "        param = json.load(f)\n",
    "    return df, param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0  お客様：こんにちは、；従業者：こんにちは、；お客様、ご機嫌いかがでしょうか？；お客様：こんに...\n",
      "(1, 1)\n",
      "{'options': {'params': {'algo': 'transformers_dialogsum_jp', 'mode': 'stage', 'epochs': '100'}, 'args': ['text'], 'feature_variables': ['text'], 'model_name': 'transformers_dialogjp_model', 'output_name': 'text', 'algo_name': 'MLTKContainer', 'mlspl_limits': {'handle_new_cat': 'default', 'max_distinct_cat_values': '100', 'max_distinct_cat_values_for_classifiers': '100', 'max_distinct_cat_values_for_scoring': '100', 'max_fit_time': '600', 'max_inputs': '100000', 'max_memory_usage_mb': '4000', 'max_model_size_mb': '30', 'max_score_time': '600', 'use_sampling': 'true'}, 'kfold_cv': None}, 'feature_variables': ['text']}\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing purposes\n",
    "df, param = stage(\"transformers_dialogjp_model\")\n",
    "print(df)\n",
    "print(df.shape)\n",
    "print(str(param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 - create and initialize a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "name": "mltkc_init"
   },
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "# params: data and parameters\n",
    "# returns the model object which will be used as a reference to call fit, apply and summary subsequently\n",
    "def init(df,param):\n",
    "    # Load English parser and text blob (for sentiment analysis)\n",
    "    print(\"debug: start model initialization\")\n",
    "    model = {}\n",
    "#     model[\"tokenizer\"] = AutoTokenizer.from_pretrained(\"philschmid/bart-large-cnn-samsum\")\n",
    "#     model[\"summarizer\"] = TFAutoModelForSeq2SeqLM.from_pretrained(\"philschmid/bart-large-cnn-samsum\")\n",
    "    model[\"tokenizer\"] = T5Tokenizer.from_pretrained(\"/srv/app/model/data/t5_dialog_jp\")\n",
    "    model[\"summarizer\"] = T5ForConditionalGeneration.from_pretrained(\"/srv/app/model/data/t5_dialog_jp\")\n",
    "    print(\"debug: model initialized\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug: start model initialization\n",
      "debug: model initialized\n"
     ]
    }
   ],
   "source": [
    "model = init(df,param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3 - fit the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for this algorithm the model is pre-trained and therefore this stage is a placeholder only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "name": "mltkc_fit"
   },
   "outputs": [],
   "source": [
    "# returns a fit info json object\n",
    "def fit(model,df,param):\n",
    "    returns = {}\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4 - apply the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "name": "mltkc_apply"
   },
   "outputs": [],
   "source": [
    "def apply(model,df,param):\n",
    "    \n",
    "    print(\"debug: start apply\")\n",
    "    \n",
    "    X = df[param['feature_variables']].values.tolist()\n",
    "    temp_data=list()\n",
    "    \n",
    "#     tokenizer = AutoTokenizer.from_pretrained(\"/srv/app/model/data/samsum_tok\")\n",
    "#     summarizer = AutoModelForSeq2SeqLM.from_pretrained(\"/srv/app/model/data/samsum\")\n",
    "    print(\"debug: read input\")\n",
    "    print(\"numpy version: \" + np.__version__)\n",
    "    for i in range(len(X)):\n",
    "        print(\"debug: start tokenizing\")\n",
    "        batch = model[\"tokenizer\"](str(X[i]), max_length=400, truncation=True, return_tensors=\"pt\")\n",
    "        \n",
    "        print(\"debug: finish tok and start summarizing\")\n",
    "        \n",
    "        outputs = model[\"summarizer\"].generate(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], max_length=400,repetition_penalty=8.0,num_beams=15)\n",
    "        \n",
    "        print(\"debug: finish summarizing and start decoding\")\n",
    "        \n",
    "        summary = [model[\"tokenizer\"].decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=False) for ids in outputs][0]\n",
    "        \n",
    "        print(\"debug: finish decoding\")\n",
    "        \n",
    "        temp_data.append(summary)\n",
    "        \n",
    "    column_names=[\"summaries\"]\n",
    "    returns=pd.DataFrame(temp_data, columns=column_names)\n",
    "    print(\"debug: finished\")\n",
    "        \n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug: start apply\n",
      "debug: read input\n",
      "numpy version: 1.22.1\n",
      "debug: start tokenizing\n",
      "debug: finish tok and start summarizing\n",
      "debug: finish summarizing and start decoding\n",
      "debug: finish decoding\n",
      "debug: finished\n",
      "無線のプレミアムバージョンに加入しているユーザー番号は100145。通常加入よりも高速であるはずなのに、とても遅いという。お客様が契約したパッケージの説明とまったく同じだそう\n"
     ]
    }
   ],
   "source": [
    "returns = apply(model,df,param)\n",
    "print(returns[\"summaries\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 5 - save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "name": "mltkc_save"
   },
   "outputs": [],
   "source": [
    "# save model to name in expected convention \"<algo_name>_<model_name>.h5\"\n",
    "def save(model,name):\n",
    "    # model will not be saved or reloaded as it is pre-built\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 6 - load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "name": "mltkc_load"
   },
   "outputs": [],
   "source": [
    "# load model from name in expected convention \"<algo_name>_<model_name>.h5\"\n",
    "def load(name):\n",
    "    # model will not be saved or reloaded as it is pre-built\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 7 - provide a summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "name": "mltkc_summary"
   },
   "outputs": [],
   "source": [
    "# return model summary\n",
    "def summary(model=None):\n",
    "    returns = {}\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Stages\n",
    "All subsequent cells are not tagged and can be used for further freeform code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
